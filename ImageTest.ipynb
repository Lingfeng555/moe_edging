{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from utils.Loader import NEUDataset\n",
    "from utils.Perspectiver import Perspectiver\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import multiprocessing\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFilter(torch.nn.Module):\n",
    "    def __init__(self, kernel_size: int, sigma: float):\n",
    "        super(GaussianFilter, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.sigma = sigma\n",
    "        self.kernel = self._create_gaussian_kernel(kernel_size, sigma)\n",
    "    \n",
    "    def _create_gaussian_kernel(self, kernel_size: int, sigma: float):\n",
    "        \"\"\"Creates a 2D Gaussian kernel.\"\"\"\n",
    "        x = torch.arange(kernel_size) - kernel_size // 2\n",
    "        y = torch.arange(kernel_size) - kernel_size // 2\n",
    "        xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "        kernel = torch.exp(-(xx**2 + yy**2) / (2 * sigma**2))\n",
    "        kernel = kernel / kernel.sum()\n",
    "        return kernel\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Applies the Gaussian filter to an input tensor.\"\"\"\n",
    "        if x.ndim == 2:  # Add batch and channel dimensions if missing\n",
    "            x = x.unsqueeze(0).unsqueeze(0)  # Shape [1, 1, H, W]\n",
    "        \n",
    "        kernel = self.kernel.to(x.device).unsqueeze(0).unsqueeze(0)  # Shape [1, 1, K, K]\n",
    "        x = F.conv2d(x, kernel, padding=self.kernel_size // 2)\n",
    "        return x.squeeze(0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = NEUDataset(set=\"train\", transform=None, seed=1, scale=0.5)\n",
    "\n",
    "# Probar con un DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Intentar iterar sobre el dataloader\n",
    "for images, labels in dataloader:\n",
    "    print(\"Iteración exitosa.\")\n",
    "    print(\"Imagen shape:\", images.shape)\n",
    "    print(\"Etiqueta shape:\", labels.shape)\n",
    "    break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = dataset.__getitem__(index=random.randint(2, len(dataset)))\n",
    "original_image = Perspectiver.grayscale_to_rgb(Perspectiver.normalize_to_uint8(image.detach().cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_image(image, n_tries: int = 100):\n",
    "    \"\"\"\n",
    "    Optimizes the mean shift parameters to maximize the metric: (Silhouette Score / Number of Clusters).\n",
    "\n",
    "    Args:\n",
    "        image (np.array): The input image with shape (200, 200, 3).\n",
    "        n_tries (int): The number of optimization trials to run.\n",
    "\n",
    "    Returns:\n",
    "        dict: The best parameters and the best score.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Suggest parameters for spatial radius and color radius\n",
    "        sp = trial.suggest_float(\"sp\", 1.0, 100.0)  # Spatial window radius\n",
    "        sr = trial.suggest_float(\"sr\", 1.0, 100.0)  # Color window radius\n",
    "\n",
    "        # Apply mean shift filtering\n",
    "        after = Perspectiver.meanShift(image, sp, sr)\n",
    "\n",
    "        # Evaluate clustering\n",
    "        scores = Perspectiver.evaluate_clustering(image, after)\n",
    "        score = scores[\"Davies-Bouldin Index\"]\n",
    "        n_clusters = len(np.unique(after))\n",
    "\n",
    "        # Avoid division by zero (in case of degenerate clustering)\n",
    "        if n_clusters == 0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        # Metric to maximize: Silhouette Score per cluster\n",
    "        metric_to_maximize = math.log2(score)*n_clusters\n",
    "        return metric_to_maximize\n",
    "\n",
    "    # Determine the number of CPU threads\n",
    "    n_jobs = multiprocessing.cpu_count()-2\n",
    "    print(f\"Using {n_jobs} CPU threads for parallel optimization.\")\n",
    "\n",
    "    # Run the optimization with parallel trials\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_tries, n_jobs=n_jobs)\n",
    "\n",
    "    # Return the best parameters and the best score\n",
    "    return {\n",
    "        \"best_params\": study.best_params,\n",
    "        \"best_score\": study.best_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barchartImage(image):\n",
    "    x = np.arange(image.shape[0])\n",
    "    y = np.arange(image.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    # Flatten arrays for plotting\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = np.zeros_like(x)\n",
    "    dx = dy = np.ones_like(x)\n",
    "    dz = image.flatten()\n",
    "\n",
    "    # Plot the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.bar3d(x, y, z, dx, dy, dz, shade=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title('3D Bar Chart of (200, 200) Array')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST = optimize_image(original_image, n_tries=200)[\"best_params\"]\n",
    "BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_barchartImage(Perspectiver.rgb_to_grayscale(original_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_image = Perspectiver.meanShift(original_image, BEST[\"sp\"], BEST[\"sr\"])\n",
    "Perspectiver.plotComparison(original_image, clustered_image, titleBefore=label)\n",
    "plot_barchartImage(Perspectiver.rgb_to_grayscale(clustered_image))\n",
    "Perspectiver.evaluate_clustering(original_image, clustered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nl_means_opencv(image: np.ndarray, h: int = 10, template_window_size: int = 7, search_window_size: int = 21):\n",
    "    \"\"\"\n",
    "    Aplica filtro Non-Local Means usando la implementación de OpenCV (CPU).\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Imagen de entrada (BGR).\n",
    "        h (int): Fuerza de filtrado (mayor -> filtrado más fuerte).\n",
    "        template_window_size (int): Tamaño del parche usado en comparación.\n",
    "        search_window_size (int): Tamaño del área para buscar parches similares.\n",
    "    Returns:\n",
    "        np.ndarray: Imagen filtrada.\n",
    "    \"\"\"\n",
    "    if not isinstance(image, np.ndarray):\n",
    "        raise TypeError(\"La imagen debe ser un arreglo NumPy.\")\n",
    "    # fastNlMeansDenoisingColored trabaja en BGR y retorna BGR\n",
    "    filtered = cv2.fastNlMeansDenoisingColored(\n",
    "        src=image,\n",
    "        h=h,\n",
    "        hColor=h,\n",
    "        templateWindowSize=template_window_size,\n",
    "        searchWindowSize=search_window_size\n",
    "    )\n",
    "    return filtered\n",
    "\n",
    "def optimize_NL_Filter(image, n_tries: int = 100):\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Suggest parameters for spatial radius and color radius\n",
    "        h = trial.suggest_int(\"h\", 0, 255)  # Spatial window radius\n",
    "        tws = trial.suggest_int(\"template_window_size\", 1.0, 60.0)  # Color window radius\n",
    "        sws = trial.suggest_int(\"search_window_size\", 1.0, 60.0)  # Color window radius\n",
    "\n",
    "        # Apply mean shift filtering\n",
    "        after = nl_means_opencv(image, h, tws, sws)\n",
    "\n",
    "        # Evaluate clustering\n",
    "        scores = Perspectiver.evaluate_clustering(image, after)\n",
    "        score = scores[\"Davies-Bouldin Index\"]\n",
    "        n_clusters = len(np.unique(after))\n",
    "\n",
    "        # Avoid division by zero (in case of degenerate clustering)\n",
    "        if n_clusters == 0:\n",
    "            return float(\"-inf\")\n",
    "\n",
    "        # Metric to maximize: Silhouette Score per cluster\n",
    "        metric_to_maximize = math.log2(score)*n_clusters\n",
    "        return metric_to_maximize\n",
    "\n",
    "    # Determine the number of CPU threads\n",
    "    n_jobs = multiprocessing.cpu_count()-2\n",
    "    print(f\"Using {n_jobs} CPU threads for parallel optimization.\")\n",
    "\n",
    "    # Run the optimization with parallel trials\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_tries, n_jobs=n_jobs)\n",
    "\n",
    "    # Return the best parameters and the best score\n",
    "    return {\n",
    "        \"best_params\": study.best_params,\n",
    "        \"best_score\": study.best_value\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST = optimize_NL_Filter(original_image, n_tries=200)[\"best_params\"]\n",
    "BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_image = nl_means_opencv(original_image, h= BEST[\"h\"], template_window_size=BEST[\"template_window_size\"], search_window_size=BEST[\"search_window_size\"])\n",
    "Perspectiver.plotComparison(original_image, clustered_image, titleBefore=label)\n",
    "plot_barchartImage(Perspectiver.rgb_to_grayscale(clustered_image))\n",
    "Perspectiver.evaluate_clustering(original_image, clustered_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
