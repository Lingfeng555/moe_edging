{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../') \n",
    "from utils.Loader import CXR8Dataset\n",
    "from utils.evaluator import Evaluator\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MoE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardGatingNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts, top_k=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): Dimension of input features.\n",
    "            num_experts (int): Total number of experts.\n",
    "            top_k (int): Maximum number of experts to activate (hard gating).\n",
    "        \"\"\"\n",
    "        super(HardGatingNetwork, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the gating network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, input_dim).\n",
    "        \n",
    "        Returns:\n",
    "            selected_experts (torch.Tensor): Binary mask of activated experts (batch_size, num_experts).\n",
    "            expert_weights (torch.Tensor): Normalized weights for the selected experts (batch_size, num_experts).\n",
    "        \"\"\"\n",
    "        logits = self.gate(x)\n",
    "        top_k_values, top_k_indices = torch.topk(logits, self.top_k, dim=1)\n",
    "\n",
    "        mask = torch.zeros_like(logits)\n",
    "        mask.scatter_(1, top_k_indices, 1.0)\n",
    "\n",
    "        sparse_logits = mask * logits \n",
    "        expert_weights = F.softmax(sparse_logits, dim=1) \n",
    "\n",
    "        return mask, expert_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ExpertCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ExpertCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        \n",
    "        # Placeholder for dynamically calculated in_features\n",
    "        self.fc = None  # Dynamically initialized in forward pass\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        if self.fc is None:\n",
    "            num_features = x.view(x.size(0), -1).size(1)\n",
    "            self.fc = nn.Linear(num_features, 20).to(x.device)  # Output size is 20\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)\n",
    "20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEModel(nn.Module):\n",
    "    def __init__(self, num_experts):\n",
    "        super(MoEModel, self).__init__()\n",
    "        self.num_experts = num_experts\n",
    "        \n",
    "        # Experts\n",
    "        self.experts = nn.ModuleList([ExpertCNN() for _ in range(num_experts)])\n",
    "        \n",
    "        # Feature extractor for gating\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Changed in_channels to 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 112 * 112, 128),  # Adjust size based on your input image dimensions\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Gating network\n",
    "        self.gating = HardGatingNetwork(input_dim=128, num_experts=num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract features from the image for gating\n",
    "        features = self.feature_extractor(x)\n",
    "        \n",
    "        # Get gating scores\n",
    "        mask, gating_scores = self.gating(features)\n",
    "        \n",
    "        # Top-2 sparsity\n",
    "        topk_values, topk_indices = torch.topk(gating_scores, k=self.num_experts, dim=-1)\n",
    "        \n",
    "        # Compute outputs for all experts\n",
    "        outputs = torch.stack([self.experts[i](x) for i in range(self.num_experts)], dim=1)\n",
    "        \n",
    "        # Select the outputs of the top-k experts\n",
    "        selected_outputs = outputs.gather(\n",
    "            1, topk_indices.unsqueeze(-1).expand(-1, -1, outputs.size(-1))\n",
    "        )\n",
    "        \n",
    "        # Combine the outputs of the selected experts\n",
    "        combined_output = (selected_outputs * topk_values.unsqueeze(-1)).sum(dim=1)\n",
    "        \n",
    "        return combined_output, gating_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = os.path.expanduser(\"~/datasets/CXR8/LongTailCXR/nih-cxr-lt_single-label_train.csv\")\n",
    "train_image_dir = os.path.expanduser(\"~/datasets/CXR8/images/images_001/images/\")\n",
    "test_csv_path = os.path.expanduser(\"~/datasets/CXR8/LongTailCXR/nih-cxr-lt_single-label_test.csv\")\n",
    "train_image_dir = os.path.expanduser(\"~/datasets/CXR8/images/images_001/images/\")\n",
    "batch_size = 16\n",
    "image_scale = (224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert image to grayscale (1 channel)\n",
    "    transforms.Resize(image_scale),               # Resize images to a uniform size\n",
    "    transforms.ToTensor(),                       # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize for grayscale (mean and std for a single channel)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CXR8Dataset(csv_path=train_csv_path, image_dir=train_image_dir, transform=transform)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "test_dataset = CXR8Dataset(csv_path=test_csv_path, image_dir=train_image_dir, transform=transform)\n",
    "test_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the filtered dataset: 2895\n",
      "Batch of images: torch.Size([16, 1, 224, 224])\n",
      "Batch of labels: torch.Size([16, 20])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of images in the filtered dataset: {len(train_dataset)}\")\n",
    "\n",
    "# Test the DataLoader\n",
    "for images, labels in train_data_loader:\n",
    "    print(f\"Batch of images: {images.shape}\")\n",
    "    print(f\"Batch of labels: {labels.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_experts = 20\n",
    "model = MoEModel(num_experts=num_experts).to('cuda')\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 0.4269, Sparsity: 0.9500\n",
      "Epoch [2/15], Loss: 0.2352, Sparsity: 0.9500\n",
      "Epoch [3/15], Loss: 0.1954, Sparsity: 0.9500\n",
      "Epoch [4/15], Loss: 0.1720, Sparsity: 0.9500\n",
      "Epoch [5/15], Loss: 0.1583, Sparsity: 0.9500\n",
      "Epoch [6/15], Loss: 0.1501, Sparsity: 0.9500\n",
      "Epoch [7/15], Loss: 0.1446, Sparsity: 0.9500\n",
      "Epoch [8/15], Loss: 0.1411, Sparsity: 0.9500\n",
      "Epoch [9/15], Loss: 0.1382, Sparsity: 0.9500\n",
      "Epoch [10/15], Loss: 0.1362, Sparsity: 0.9500\n",
      "Epoch [11/15], Loss: 0.1343, Sparsity: 0.9500\n",
      "Epoch [12/15], Loss: 0.1325, Sparsity: 0.9500\n",
      "Epoch [13/15], Loss: 0.1309, Sparsity: 0.9500\n",
      "Epoch [14/15], Loss: 0.1294, Sparsity: 0.9500\n",
      "Epoch [15/15], Loss: 0.1280, Sparsity: 0.9500\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_sparsity = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, labels in train_data_loader:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, gating_scores = model(images)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)  # Multi-label loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Calculate sparsity\n",
    "        sparsity = 1 - (gating_scores.sum(dim=1) / model.num_experts).mean().item()\n",
    "        total_sparsity += sparsity\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / len(train_data_loader)\n",
    "    avg_sparsity = total_sparsity / num_batches\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Sparsity: {avg_sparsity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categs = test_data_loader.dataset.data.columns.to_list()\n",
    "categs.remove(\"id\")\n",
    "categs.remove(\"subject_id\")\n",
    "len(categs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addnewrow(dataframe, row_list):\n",
    "    \"\"\"\n",
    "    Agrega una nueva fila a un DataFrame dado a partir de una lista.\n",
    "    \n",
    "    Parámetros:\n",
    "    - dataframe: pd.DataFrame. El DataFrame al que se agregará la nueva fila.\n",
    "    - row_list: list. Una lista que contiene los valores de la nueva fila.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: El DataFrame actualizado con la nueva fila.\n",
    "    \"\"\"\n",
    "    if len(row_list) != len(dataframe.columns):\n",
    "        raise ValueError(\"La longitud de la lista no coincide con el número de columnas del DataFrame\")\n",
    "    \n",
    "    # Convertimos la lista en un DataFrame temporal\n",
    "    new_row = pd.DataFrame([row_list], columns=dataframe.columns)\n",
    "    \n",
    "    # Concatenamos el DataFrame temporal con el original\n",
    "    dataframe = pd.concat([dataframe, new_row], ignore_index=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21532/1138308564.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, new_row], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "\n",
    "df_test = pd.DataFrame(columns=categs)\n",
    "df_pred = pd.DataFrame(columns=categs)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_data_loader:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda')\n",
    "        outputs, _ = model(images)\n",
    "        \n",
    "        # Apply sigmoid to outputs and threshold at 0.5\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        #print(len(predictions.detach().cpu().numpy().tolist()))\n",
    "        print(len(labels.detach().cpu().numpy().tolist()[0]))\n",
    "        # Collect predictions and labels\n",
    "\n",
    "        for x in predictions:\n",
    "            df_pred = addnewrow(df_pred, x.cpu().numpy().tolist())\n",
    "        for x in labels:\n",
    "            df_test = addnewrow(df_test, x.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9765112262521589 \n",
      " Precision: 0.9549084891349318 \n",
      " Recall: 0.9765112262521589 \n",
      " F1 Score: 0.9655890454622011\n",
      "Confusion Matrix:\n",
      " [[2827    2]\n",
      " [  66    0]]\n",
      "ROC AUC: 0.4996465182043125\n"
     ]
    }
   ],
   "source": [
    "Evaluator.eval_classification(y_pred=df_pred[\"Fibrosis\"], y_true=df_test['Fibrosis'], binary_classification=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
