{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingfeng/Desktop/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "from utils.Loader import NEUDataset\n",
    "from utils.Perspectiver import Perspectiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida final unbatched: torch.Size([1, 2])\n",
      "Salida final batched: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class RL_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RL_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 144, 2)  # 3 poolings reducen 100 -> 50 -> 25 -> 12\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:  # (batch_size, 100, 100)\n",
    "            x = x.unsqueeze(1) \n",
    "        # x: [1, 100, 100]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)                   # 100 -> 50\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)                   # 50 -> 25\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool(x)                   # 25 -> 12\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)          # [1, 32*12*12]\n",
    "        # print(x.shape)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def save(self, path=\"rl_cnn.pth\"):\n",
    "        \"\"\"Guarda los pesos del modelo en un archivo.\"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Modelo guardado en {path}\")\n",
    "    \n",
    "    def load(self, path=\"rl_cnn.pth\"):\n",
    "        \"\"\"Carga los pesos del modelo desde un archivo.\"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        print(f\"Modelo cargado desde {path}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## test\n",
    "    modelo = RL_CNN()\n",
    "    entrada = torch.randn(1, 100, 100)\n",
    "    salida = modelo(entrada)\n",
    "    print(\"Salida final unbatched:\", salida.shape)  # -> [1, 2]\n",
    "    entrada = torch.randn(32, 1, 100, 100)\n",
    "    salida = modelo(entrada)\n",
    "    print(\"Salida final batched:\", salida.shape)  # -> [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\" Save a transition (state, action, reward, next_state, done) \"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample a batch of experiences \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_Agent:\n",
    "    def __init__(self, gamma=0.99, lr=1e-3, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        # Neural Network\n",
    "        self.model = RL_CNN()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Experience Replay Memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Predicts two floating numbers instead of discrete action selection \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = self.model(state)\n",
    "        return output  # Output: Two continuous numbers\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        \"\"\" Save an experience tuple (state, output, reward, next_state, done) \"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def train_step(self, batch_size=32):\n",
    "        \"\"\" Train using a batch from experience replay \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, outputs, rewards, next_states, dones = zip(*batch)\n",
    "        #print(states[0].shape)\n",
    "        states = torch.stack(states)\n",
    "        next_states = torch.stack(next_states)\n",
    "        outputs = torch.stack(outputs).squeeze(1)\n",
    "        rewards = torch.tensor(rewards).float().unsqueeze(1)\n",
    "        dones = torch.tensor(dones).float().unsqueeze(1)\n",
    "\n",
    "        # Compute target using Bellman equation\n",
    "        next_outputs = self.model(next_states)\n",
    "        \n",
    "        target_values = (outputs + (1 - dones) * self.gamma * next_outputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(outputs, target_values)\n",
    "\n",
    "        # Optimize model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 3. Define Environment for NEU Dataset with New Reward Function\n",
    "# ------------------------------\n",
    "class NEUEnvironment:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.index = 0  # Track current index\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset environment to initial state \"\"\"\n",
    "        self.index = 1\n",
    "        return self.dataset[self.index][0]  # Return first image\n",
    "\n",
    "    def step(self, output, n_jobs = 1):\n",
    "        \"\"\" Simulate environment response to action (2 float values) \"\"\"\n",
    "        image, label = self.dataset[self.index]\n",
    "        reward = self.reward_function(output, image)\n",
    "        self.index = (self.index + 1)%len(self.dataset)  # Move to next sample\n",
    "        next_state = self.dataset[self.index][0]  # Get next image\n",
    "        #print(f\"The next input is: {next_state.shape}\")\n",
    "        done = self.index == 0  # End of epoch\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def calculate_reward(self, image, sp, sr):\n",
    "        image = Perspectiver.grayscale_to_rgb(Perspectiver.normalize_to_uint8(image.detach().cpu().numpy()[0]))\n",
    "        after = Perspectiver.meanShift(image, sp, sr)\n",
    "        original_gray = Perspectiver.rgb_to_grayscale(image).flatten()\n",
    "        clustered_gray = Perspectiver.rgb_to_grayscale(after).flatten()\n",
    "        score = davies_bouldin_score(original_gray.reshape(-1, 1), clustered_gray)\n",
    "\n",
    "        n_clusters = len(np.unique(after))\n",
    "\n",
    "        # Avoid division by zero (in case of degenerate clustering)\n",
    "        if n_clusters == 0:\n",
    "            return -10000\n",
    "\n",
    "        # Metric to maximize: Silhouette Score per cluster\n",
    "        return math.log2(score)*n_clusters\n",
    "\n",
    "\n",
    "    def reward_function(self, output, image):\n",
    "        \"\"\" Reward function based or the distance between the predicted values and correct values \"\"\"\n",
    "        output = output.detach().cpu().numpy()\n",
    "        sp = output[0][0]\n",
    "        sr = output[0][1]\n",
    "        if (sp <= 0) : return 2000 * sp\n",
    "        if (sr <= 0) : return 2000 * sr\n",
    "        if sp > sr : return -5000\n",
    "        return self.calculate_reward(image, sp, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_model(num_episodes=1000, batch_size=32):\n",
    "    dataset = NEUDataset(set=\"train\", scale=0.5)\n",
    "    env = NEUEnvironment(dataset)\n",
    "    agent = RL_Agent(gamma=0.99, lr=1e-2, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.05)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            output = agent.select_action(state)\n",
    "            next_state, reward, done = env.step(output)\n",
    "\n",
    "            agent.store_experience((state, output, reward, next_state, done))\n",
    "            #print(f\"state: {state.shape}\")\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            agent.train_step(batch_size)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    agent.model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rl_model(num_episodes=10, batch_size=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
