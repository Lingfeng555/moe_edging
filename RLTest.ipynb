{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lingfeng/Desktop/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import concurrent.futures\n",
    "\n",
    "from utils.Loader import NEUDataset\n",
    "from utils.Perspectiver import Perspectiver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida final unbatched: torch.Size([1, 2])\n",
      "Salida final batched: torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "class RL_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RL_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 8, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(8, 16, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(32 * 144, 2)  # 3 poolings reducen 100 -> 50 -> 25 -> 12\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 3:  # (batch_size, 100, 100)\n",
    "            x = x.unsqueeze(1) \n",
    "        # x: [1, 100, 100]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)                   # 100 -> 50\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool(x)                   # 50 -> 25\n",
    "        # print(x.shape)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.pool(x)                   # 25 -> 12\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)          # [1, 32*12*12]\n",
    "        # print(x.shape)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    def save(self, path=\"rl_cnn.pth\"):\n",
    "        \"\"\"Guarda los pesos del modelo en un archivo.\"\"\"\n",
    "        torch.save(self.state_dict(), path)\n",
    "        print(f\"Modelo guardado en {path}\")\n",
    "    \n",
    "    def load(self, path=\"rl_cnn.pth\"):\n",
    "        \"\"\"Carga los pesos del modelo desde un archivo.\"\"\"\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        print(f\"Modelo cargado desde {path}\")\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## test\n",
    "    modelo = RL_CNN()\n",
    "    entrada = torch.randn(1, 100, 100)\n",
    "    salida = modelo(entrada)\n",
    "    print(\"Salida final unbatched:\", salida.shape)  # -> [1, 2]\n",
    "    entrada = torch.randn(32, 1, 100, 100)\n",
    "    salida = modelo(entrada)\n",
    "    print(\"Salida final batched:\", salida.shape)  # -> [1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, transition):\n",
    "        \"\"\" Save a transition (state, action, reward, next_state, done) \"\"\"\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\" Sample a batch of experiences \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_Agent:\n",
    "    def __init__(self, gamma=0.99, lr=1e-3, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "\n",
    "        # Neural Network\n",
    "        self.model = RL_CNN()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        # Experience Replay Memory\n",
    "        self.memory = deque(maxlen=10000)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\" Predicts two floating numbers instead of discrete action selection \"\"\"\n",
    "        with torch.no_grad():\n",
    "            output = self.model(state)\n",
    "        return output  # Output: Two continuous numbers\n",
    "\n",
    "    def store_experience(self, experience):\n",
    "        \"\"\" Save an experience tuple (state, output, reward, next_state, done) \"\"\"\n",
    "        self.memory.append(experience)\n",
    "\n",
    "    def train_step(self, batch_size=32):\n",
    "        \"\"\" Train using a batch from experience replay \"\"\"\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        states, outputs, rewards, next_states, dones = zip(*batch)\n",
    "        #print(states[0].shape)\n",
    "        states = torch.stack(states)\n",
    "        next_states = torch.stack(next_states)\n",
    "        outputs = torch.stack(outputs).squeeze(1)\n",
    "        rewards = torch.tensor(rewards).float().unsqueeze(1)\n",
    "        dones = torch.tensor(dones).float().unsqueeze(1)\n",
    "\n",
    "        # Compute target using Bellman equation\n",
    "        next_outputs = self.model(next_states)\n",
    "        \n",
    "        target_values = (outputs + (1 - dones) * self.gamma * next_outputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = self.loss_fn(outputs, target_values)\n",
    "\n",
    "        # Optimize model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# 3. Define Environment for NEU Dataset with New Reward Function\n",
    "# ------------------------------\n",
    "class NEUEnvironment:\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.index = 0  # Track current index\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Reset environment to initial state \"\"\"\n",
    "        self.index = 1\n",
    "        return self.dataset[self.index][0]  # Return first image\n",
    "\n",
    "    def step(self, output, n_jobs = 1):\n",
    "        \"\"\" Simulate environment response to action (2 float values) \"\"\"\n",
    "        image, label = self.dataset[self.index]\n",
    "        reward = self.reward_function(output, image)\n",
    "        self.index = (self.index + 1)%len(self.dataset)  # Move to next sample\n",
    "        next_state = self.dataset[self.index][0]  # Get next image\n",
    "        #print(f\"The next input is: {next_state.shape}\")\n",
    "        done = self.index == 0  # End of epoch\n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def calculate_reward(self, image, sp, sr):\n",
    "        image = Perspectiver.grayscale_to_rgb(Perspectiver.normalize_to_uint8(image.detach().cpu().numpy()[0]))\n",
    "        after = Perspectiver.meanShift(image, sp, sr)\n",
    "        original_gray = Perspectiver.rgb_to_grayscale(image).flatten()\n",
    "        clustered_gray = Perspectiver.rgb_to_grayscale(after).flatten()\n",
    "        score = davies_bouldin_score(original_gray.reshape(-1, 1), clustered_gray)\n",
    "\n",
    "        n_clusters = len(np.unique(after))\n",
    "\n",
    "        # Avoid division by zero (in case of degenerate clustering)\n",
    "        if n_clusters == 0:\n",
    "            return -10000\n",
    "\n",
    "        # Metric to maximize: Silhouette Score per cluster\n",
    "        return math.log2(score)*n_clusters\n",
    "\n",
    "\n",
    "    def reward_function(self, output, image):\n",
    "        \"\"\" Reward function based or the distance between the predicted values and correct values \"\"\"\n",
    "        output = output.detach().cpu().numpy()\n",
    "        sp = output[0][0]\n",
    "        sr = output[0][1]\n",
    "        if (sp <= 0) : return 2000 * (sp-1)\n",
    "        if (sr <= 0) : return 2000 * (sr-1)\n",
    "        if sp > sr : return -5000\n",
    "        return self.calculate_reward(image, sp, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_model(num_episodes=1000, batch_size=32, n_jobs=4):\n",
    "    \"\"\"\n",
    "    Entrena el modelo usando múltiples hilos en el paso del entorno.\n",
    "    \"\"\"\n",
    "    dataset = NEUDataset(set=\"train\", scale=0.5)\n",
    "    env = NEUEnvironment(dataset)\n",
    "    agent = RL_Agent(gamma=0.99, lr=1e-2, epsilon=1.0, epsilon_decay=0.995, min_epsilon=0.05)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # Executor para paralelizar pasos cuando se requiera.\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            while not done:\n",
    "                output = agent.select_action(state)\n",
    "                \n",
    "                # Llamada asíncrona a step; se puede esperar su resultado inmediatamente\n",
    "                future_step = executor.submit(env.step, output)\n",
    "                next_state, reward, done = future_step.result()\n",
    "                \n",
    "                agent.store_experience((state, output, reward, next_state, done))\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                agent.train_step(batch_size)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}, \"\n",
    "              f\"Total Reward: {total_reward:.3f}, Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    agent.model.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/10, Total Reward: 1944812.375, Epsilon: 0.050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_rl_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36mtrain_rl_model\u001b[0;34m(num_episodes, batch_size, n_jobs)\u001b[0m\n\u001b[1;32m     24\u001b[0m             state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m     25\u001b[0m             total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 26\u001b[0m             \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m           \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Epsilon: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent\u001b[38;5;241m.\u001b[39mepsilon\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m agent\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave()\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mRL_Agent.train_step\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Optimize model\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 51\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# Decay epsilon\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pytorch/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_rl_model(num_episodes=10, batch_size=32, n_jobs=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
