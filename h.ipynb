{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liingfeng/Desktop/pytorch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from utils.Loader import NEUDataset\n",
    "from utils.Perspectiver import Perspectiver\n",
    "from source.Prototype1 import Prototype1\n",
    "from source.Classifier import MetalClassifier\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import multiprocessing\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from scipy.ndimage import maximum_filter, minimum_filter, label, generate_binary_structure\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from scipy.ndimage import label as ndi_label, binary_dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda cores\n",
      "Dataset: train created!\n"
     ]
    }
   ],
   "source": [
    "model = Prototype1(num_attention_heads=16).to(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda cores\")\n",
    "    model.cuda()\n",
    "\n",
    "dataset = NEUDataset(set=\"train\", seed=555, scale=0.5, best_param=True, output_path=\"outputs_k10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = len(dataset)//2\n",
    "THREADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "Best loss: '2532'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    loss_record = []\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Desempaquetar ignorando 'labels'\n",
    "        for images, _, best_parameters in dataloader:\n",
    "            images = images.to('cuda')\n",
    "            best_parameters = best_parameters.to('cuda')\n",
    "\n",
    "            # Forward: el modelo predice sp y sr\n",
    "            pred_reg = model(images)  # salida de regresión\n",
    "\n",
    "            # Calcular la pérdida de regresión\n",
    "            loss = criterion(pred_reg, best_parameters)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        loss_record.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCH}], Loss: {avg_loss:.4f}\")\n",
    "    return loss_record, model\n",
    "\n",
    "loss, model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parametros = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)           \n",
    "plt.title(\"Line Chart\")  \n",
    "plt.xlabel(\"epoch\")     \n",
    "plt.ylabel(\"loss\")      \n",
    "plt.show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar solo la red neuronal en un archivo .pth\n",
    "torch.save(model.state_dict(), \"h2.pth\")\n",
    "print(\"Modelo guardado exitosamente en h2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Prototype1(num_attention_heads=16)\n",
    "loaded_model.load_state_dict(torch.load(\"h2.pth\", map_location=torch.device('cpu')))\n",
    "loaded_model.to(\"cuda\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barchartImage(image):\n",
    "    x = np.arange(image.shape[0])\n",
    "    y = np.arange(image.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    # Flatten arrays for plotting\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = np.zeros_like(x)\n",
    "    dx = dy = np.ones_like(x)\n",
    "    dz = image.flatten()\n",
    "\n",
    "    # Plot the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.bar3d(x, y, z, dx, dy, dz, shade=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title('3D Bar Chart of (200, 200) Array')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def use_model_to_cluster(model: nn.Module, image: torch.tensor):\n",
    "    best_parameters = model(image.to(\"cuda\"))\n",
    "    print(best_parameters)\n",
    "    sp = best_parameters[0][0]\n",
    "    if sp <= 1: sp = 1\n",
    "    sr = best_parameters[0][1]\n",
    "    if sr <= 1: sr = 1\n",
    "    return sp, sr\n",
    "\n",
    "def plot_how_the_model_is_watching_the_picture(model: nn.Module, dataset: NEUDataset):\n",
    "    image, label = dataset.__getitem__(index= random.randint(0, len(dataset)))\n",
    "    print(label)\n",
    "    original_image = Perspectiver.grayscale_to_rgb(Perspectiver.normalize_to_uint8(image.detach().cpu().numpy()[0]))\n",
    "    sp , sr = use_model_to_cluster(loaded_model, image)\n",
    "    clustered_image = Perspectiver.meanShift(original_image, float(sp), float(sr))\n",
    "    Perspectiver.plotComparison(imageBefore=original_image, imageAfter=clustered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test created!\n"
     ]
    }
   ],
   "source": [
    "test_set = NEUDataset(set=\"test\", seed=55, scale=0.5, best_param=False)\n",
    "#plot_how_the_model_is_watching_the_picture(loaded_model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train created!\n",
      "1656\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = len(dataset)//4\n",
    "THREADS = 14\n",
    "dataset = NEUDataset(set=\"train\", seed=555, scale=0.5)\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, dataloader, criterion, optimizer):\n",
    "    loss_record = []\n",
    "    classifier.train()\n",
    "    for epoch in range(EPOCH):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Desempaquetar ignorando 'labels'\n",
    "        for images, label in dataloader:\n",
    "            images = images.to('cuda')\n",
    "            label = label[:, 0:6].float().to(device)\n",
    "\n",
    "            # Forward: el modelo predice sp y sr\n",
    "            pred_reg = classifier(images)  # salida de regresión\n",
    "\n",
    "            #print(pred_reg.shape)\n",
    "            #print(label.shape)\n",
    "\n",
    "            loss = criterion(pred_reg, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "            #Opcional: imprimir para depurar\n",
    "            #print(\"Logits:\", pred_reg)\n",
    "            #print(\"Labels:\", label)\n",
    "            #print(\"Batch Loss:\", loss.item())\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        loss_record.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCH}], Loss: {avg_loss:.4f}\")\n",
    "    return loss_record, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best:  num_feature_extractors=2, features_per_extractor=6 ---> loss 24.012\n",
    "Best:  num_feature_extractors=2, features_per_extractor=8 ---> loss 3.0934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda cores\n",
      "Model parameters: 1175448\n"
     ]
    }
   ],
   "source": [
    "classifier =  MetalClassifier(output_len=6, num_feature_extractors=2, features_per_extractor=8).to(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda cores\")\n",
    "    classifier.cuda()\n",
    "\n",
    "total_parametros = sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"Model parameters: {total_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 495.1255\n",
      "Epoch [2/1000], Loss: 495.1058\n",
      "Epoch [3/1000], Loss: 495.0918\n",
      "Epoch [4/1000], Loss: 495.0735\n",
      "Epoch [5/1000], Loss: 495.0539\n",
      "Epoch [6/1000], Loss: 495.0431\n",
      "Epoch [7/1000], Loss: 495.0275\n",
      "Epoch [8/1000], Loss: 495.0099\n",
      "Epoch [9/1000], Loss: 494.9946\n",
      "Epoch [10/1000], Loss: 494.9822\n",
      "Epoch [11/1000], Loss: 494.9691\n",
      "Epoch [12/1000], Loss: 494.9567\n",
      "Epoch [13/1000], Loss: 494.9424\n",
      "Epoch [14/1000], Loss: 494.9321\n",
      "Epoch [15/1000], Loss: 494.9171\n",
      "Epoch [16/1000], Loss: 494.9064\n",
      "Epoch [17/1000], Loss: 494.8933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x73fb4d8f8220>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/liingfeng/Desktop/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/liingfeng/Desktop/pytorch/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/1000], Loss: 494.8817\n",
      "Epoch [19/1000], Loss: 494.8721\n",
      "Epoch [20/1000], Loss: 494.8590\n",
      "Epoch [21/1000], Loss: 494.8500\n",
      "Epoch [22/1000], Loss: 494.8397\n",
      "Epoch [23/1000], Loss: 494.8307\n",
      "Epoch [24/1000], Loss: 494.8201\n",
      "Epoch [25/1000], Loss: 494.8102\n",
      "Epoch [26/1000], Loss: 494.8001\n",
      "Epoch [27/1000], Loss: 494.7910\n",
      "Epoch [28/1000], Loss: 494.7809\n",
      "Epoch [29/1000], Loss: 494.7721\n",
      "Epoch [30/1000], Loss: 494.7630\n",
      "Epoch [31/1000], Loss: 494.7499\n",
      "Epoch [32/1000], Loss: 494.7354\n",
      "Epoch [33/1000], Loss: 494.7119\n",
      "Epoch [34/1000], Loss: 494.6834\n",
      "Epoch [35/1000], Loss: 494.6327\n",
      "Epoch [36/1000], Loss: 494.5558\n",
      "Epoch [37/1000], Loss: 494.4266\n",
      "Epoch [38/1000], Loss: 494.2317\n",
      "Epoch [39/1000], Loss: 493.9205\n",
      "Epoch [40/1000], Loss: 493.3940\n",
      "Epoch [41/1000], Loss: 492.4295\n",
      "Epoch [42/1000], Loss: 490.5197\n",
      "Epoch [43/1000], Loss: 486.7846\n",
      "Epoch [44/1000], Loss: 478.2816\n",
      "Epoch [45/1000], Loss: 463.9382\n",
      "Epoch [46/1000], Loss: 443.9004\n",
      "Epoch [47/1000], Loss: 427.0653\n",
      "Epoch [48/1000], Loss: 420.2417\n",
      "Epoch [49/1000], Loss: 414.3963\n",
      "Epoch [50/1000], Loss: 409.9603\n",
      "Epoch [51/1000], Loss: 406.7445\n",
      "Epoch [52/1000], Loss: 402.2493\n",
      "Epoch [53/1000], Loss: 400.5391\n",
      "Epoch [54/1000], Loss: 396.1573\n",
      "Epoch [55/1000], Loss: 393.3390\n",
      "Epoch [56/1000], Loss: 390.0242\n",
      "Epoch [57/1000], Loss: 389.3104\n",
      "Epoch [58/1000], Loss: 384.2484\n",
      "Epoch [59/1000], Loss: 380.5601\n",
      "Epoch [60/1000], Loss: 376.1642\n",
      "Epoch [61/1000], Loss: 373.1335\n",
      "Epoch [62/1000], Loss: 368.1863\n",
      "Epoch [63/1000], Loss: 363.7002\n",
      "Epoch [64/1000], Loss: 345.7721\n",
      "Epoch [65/1000], Loss: 326.9596\n",
      "Epoch [66/1000], Loss: 307.7022\n",
      "Epoch [67/1000], Loss: 297.4906\n",
      "Epoch [68/1000], Loss: 293.6881\n",
      "Epoch [69/1000], Loss: 287.0659\n",
      "Epoch [70/1000], Loss: 283.4221\n",
      "Epoch [71/1000], Loss: 273.1393\n",
      "Epoch [72/1000], Loss: 268.3595\n",
      "Epoch [73/1000], Loss: 259.2866\n",
      "Epoch [74/1000], Loss: 253.2129\n",
      "Epoch [75/1000], Loss: 249.3951\n",
      "Epoch [76/1000], Loss: 240.3610\n",
      "Epoch [77/1000], Loss: 234.6955\n",
      "Epoch [78/1000], Loss: 232.3146\n",
      "Epoch [79/1000], Loss: 226.1923\n",
      "Epoch [80/1000], Loss: 233.0664\n",
      "Epoch [81/1000], Loss: 224.3672\n",
      "Epoch [82/1000], Loss: 221.2763\n",
      "Epoch [83/1000], Loss: 213.3012\n",
      "Epoch [84/1000], Loss: 209.1302\n",
      "Epoch [85/1000], Loss: 207.6412\n",
      "Epoch [86/1000], Loss: 201.2067\n",
      "Epoch [87/1000], Loss: 197.4601\n",
      "Epoch [88/1000], Loss: 192.1336\n",
      "Epoch [89/1000], Loss: 190.9035\n",
      "Epoch [90/1000], Loss: 188.2844\n",
      "Epoch [91/1000], Loss: 185.3838\n",
      "Epoch [92/1000], Loss: 182.7940\n",
      "Epoch [93/1000], Loss: 178.6544\n",
      "Epoch [94/1000], Loss: 174.9292\n",
      "Epoch [95/1000], Loss: 172.0884\n",
      "Epoch [96/1000], Loss: 170.6159\n",
      "Epoch [97/1000], Loss: 164.6610\n",
      "Epoch [98/1000], Loss: 162.2114\n",
      "Epoch [99/1000], Loss: 157.5134\n",
      "Epoch [100/1000], Loss: 156.9307\n",
      "Epoch [101/1000], Loss: 154.5526\n",
      "Epoch [102/1000], Loss: 148.9892\n",
      "Epoch [103/1000], Loss: 147.0113\n",
      "Epoch [104/1000], Loss: 148.6065\n",
      "Epoch [105/1000], Loss: 146.6842\n",
      "Epoch [106/1000], Loss: 146.0784\n",
      "Epoch [107/1000], Loss: 137.2527\n",
      "Epoch [108/1000], Loss: 136.0444\n",
      "Epoch [109/1000], Loss: 135.9333\n",
      "Epoch [110/1000], Loss: 139.4891\n",
      "Epoch [111/1000], Loss: 150.1049\n",
      "Epoch [112/1000], Loss: 153.0125\n",
      "Epoch [113/1000], Loss: 153.8527\n",
      "Epoch [114/1000], Loss: 143.9689\n",
      "Epoch [115/1000], Loss: 130.3388\n",
      "Epoch [116/1000], Loss: 121.7861\n",
      "Epoch [117/1000], Loss: 115.4163\n",
      "Epoch [118/1000], Loss: 116.6557\n",
      "Epoch [119/1000], Loss: 111.6943\n",
      "Epoch [120/1000], Loss: 107.5137\n",
      "Epoch [121/1000], Loss: 108.5605\n",
      "Epoch [122/1000], Loss: 110.8263\n",
      "Epoch [123/1000], Loss: 111.7294\n",
      "Epoch [124/1000], Loss: 101.5468\n",
      "Epoch [125/1000], Loss: 103.4877\n",
      "Epoch [126/1000], Loss: 98.2080\n",
      "Epoch [127/1000], Loss: 105.8633\n",
      "Epoch [128/1000], Loss: 95.0046\n",
      "Epoch [129/1000], Loss: 88.5616\n",
      "Epoch [130/1000], Loss: 85.3784\n",
      "Epoch [131/1000], Loss: 85.7973\n",
      "Epoch [132/1000], Loss: 92.1684\n",
      "Epoch [133/1000], Loss: 82.5193\n",
      "Epoch [134/1000], Loss: 79.8692\n",
      "Epoch [135/1000], Loss: 78.7569\n",
      "Epoch [136/1000], Loss: 85.8300\n",
      "Epoch [137/1000], Loss: 75.6392\n",
      "Epoch [138/1000], Loss: 78.1566\n",
      "Epoch [139/1000], Loss: 76.0458\n",
      "Epoch [140/1000], Loss: 74.9850\n",
      "Epoch [141/1000], Loss: 111.3069\n",
      "Epoch [142/1000], Loss: 183.5278\n",
      "Epoch [143/1000], Loss: 121.6523\n",
      "Epoch [144/1000], Loss: 123.1530\n",
      "Epoch [145/1000], Loss: 111.8891\n",
      "Epoch [146/1000], Loss: 106.5586\n",
      "Epoch [147/1000], Loss: 93.9801\n",
      "Epoch [148/1000], Loss: 86.9119\n",
      "Epoch [149/1000], Loss: 83.9553\n",
      "Epoch [150/1000], Loss: 77.0887\n",
      "Epoch [151/1000], Loss: 74.0940\n",
      "Epoch [152/1000], Loss: 73.4798\n",
      "Epoch [153/1000], Loss: 70.1094\n",
      "Epoch [154/1000], Loss: 71.0666\n",
      "Epoch [155/1000], Loss: 62.9503\n",
      "Epoch [156/1000], Loss: 61.0627\n",
      "Epoch [157/1000], Loss: 64.7000\n",
      "Epoch [158/1000], Loss: 62.3244\n",
      "Epoch [159/1000], Loss: 65.8161\n",
      "Epoch [160/1000], Loss: 61.5679\n",
      "Epoch [161/1000], Loss: 55.4877\n",
      "Epoch [162/1000], Loss: 58.0909\n",
      "Epoch [163/1000], Loss: 59.4268\n",
      "Epoch [164/1000], Loss: 55.6156\n",
      "Epoch [165/1000], Loss: 54.4090\n",
      "Epoch [166/1000], Loss: 50.6076\n",
      "Epoch [167/1000], Loss: 56.9458\n",
      "Epoch [168/1000], Loss: 54.6153\n",
      "Epoch [169/1000], Loss: 56.1381\n",
      "Epoch [170/1000], Loss: 50.6018\n",
      "Epoch [171/1000], Loss: 50.4804\n",
      "Epoch [172/1000], Loss: 44.5656\n",
      "Epoch [173/1000], Loss: 47.5646\n",
      "Epoch [174/1000], Loss: 46.7562\n",
      "Epoch [175/1000], Loss: 48.1742\n",
      "Epoch [176/1000], Loss: 46.8455\n",
      "Epoch [177/1000], Loss: 52.0114\n",
      "Epoch [178/1000], Loss: 40.2642\n",
      "Epoch [179/1000], Loss: 46.6798\n",
      "Epoch [180/1000], Loss: 44.4328\n",
      "Epoch [181/1000], Loss: 43.5706\n",
      "Epoch [182/1000], Loss: 51.6419\n",
      "Epoch [183/1000], Loss: 49.0180\n",
      "Epoch [184/1000], Loss: 62.0149\n",
      "Epoch [185/1000], Loss: 50.8681\n",
      "Epoch [186/1000], Loss: 52.0807\n",
      "Epoch [187/1000], Loss: 49.9308\n",
      "Epoch [188/1000], Loss: 50.0524\n",
      "Epoch [189/1000], Loss: 44.2059\n",
      "Epoch [190/1000], Loss: 55.0061\n",
      "Epoch [191/1000], Loss: 42.4475\n",
      "Epoch [192/1000], Loss: 40.7192\n",
      "Epoch [193/1000], Loss: 51.9199\n",
      "Epoch [194/1000], Loss: 44.5787\n",
      "Epoch [195/1000], Loss: 50.4103\n",
      "Epoch [196/1000], Loss: 48.4946\n",
      "Epoch [197/1000], Loss: 44.0777\n",
      "Epoch [198/1000], Loss: 43.2315\n",
      "Epoch [199/1000], Loss: 49.7979\n",
      "Epoch [200/1000], Loss: 41.1477\n",
      "Epoch [201/1000], Loss: 40.3932\n",
      "Epoch [202/1000], Loss: 42.2236\n",
      "Epoch [203/1000], Loss: 38.5703\n",
      "Epoch [204/1000], Loss: 39.8785\n",
      "Epoch [205/1000], Loss: 38.7456\n",
      "Epoch [206/1000], Loss: 39.6005\n",
      "Epoch [207/1000], Loss: 36.8370\n",
      "Epoch [208/1000], Loss: 34.1146\n",
      "Epoch [209/1000], Loss: 38.6221\n",
      "Epoch [210/1000], Loss: 38.8714\n",
      "Epoch [211/1000], Loss: 38.0050\n",
      "Epoch [212/1000], Loss: 32.8691\n",
      "Epoch [213/1000], Loss: 42.2101\n",
      "Epoch [214/1000], Loss: 36.7857\n",
      "Epoch [215/1000], Loss: 36.8447\n",
      "Epoch [216/1000], Loss: 35.2577\n",
      "Epoch [217/1000], Loss: 33.5130\n",
      "Epoch [218/1000], Loss: 34.0149\n",
      "Epoch [219/1000], Loss: 35.6872\n",
      "Epoch [220/1000], Loss: 38.4725\n",
      "Epoch [221/1000], Loss: 40.5192\n",
      "Epoch [222/1000], Loss: 30.9421\n",
      "Epoch [223/1000], Loss: 30.9855\n",
      "Epoch [224/1000], Loss: 34.8851\n",
      "Epoch [225/1000], Loss: 27.5964\n",
      "Epoch [226/1000], Loss: 39.9964\n",
      "Epoch [227/1000], Loss: 56.2198\n",
      "Epoch [228/1000], Loss: 55.5112\n",
      "Epoch [229/1000], Loss: 49.7480\n",
      "Epoch [230/1000], Loss: 41.7628\n",
      "Epoch [231/1000], Loss: 48.2287\n",
      "Epoch [232/1000], Loss: 44.0865\n",
      "Epoch [233/1000], Loss: 44.1571\n",
      "Epoch [234/1000], Loss: 38.0908\n",
      "Epoch [235/1000], Loss: 35.2623\n",
      "Epoch [236/1000], Loss: 32.2717\n",
      "Epoch [237/1000], Loss: 29.7007\n",
      "Epoch [238/1000], Loss: 30.2049\n",
      "Epoch [239/1000], Loss: 36.2794\n",
      "Epoch [240/1000], Loss: 36.9241\n",
      "Epoch [241/1000], Loss: 45.1540\n",
      "Epoch [242/1000], Loss: 37.4186\n",
      "Epoch [243/1000], Loss: 36.3820\n",
      "Epoch [244/1000], Loss: 43.5586\n",
      "Epoch [245/1000], Loss: 34.2167\n",
      "Epoch [246/1000], Loss: 36.4817\n",
      "Epoch [247/1000], Loss: 33.1581\n",
      "Epoch [248/1000], Loss: 39.8086\n",
      "Epoch [249/1000], Loss: 32.7292\n",
      "Epoch [250/1000], Loss: 35.5191\n",
      "Epoch [251/1000], Loss: 31.6243\n",
      "Epoch [252/1000], Loss: 30.7638\n",
      "Epoch [253/1000], Loss: 31.8123\n",
      "Epoch [254/1000], Loss: 29.9219\n",
      "Epoch [255/1000], Loss: 30.5088\n",
      "Epoch [256/1000], Loss: 31.1036\n",
      "Epoch [257/1000], Loss: 33.5848\n",
      "Epoch [258/1000], Loss: 41.6730\n",
      "Epoch [259/1000], Loss: 35.5920\n",
      "Epoch [260/1000], Loss: 35.1764\n",
      "Epoch [261/1000], Loss: 32.9138\n",
      "Epoch [262/1000], Loss: 26.4337\n",
      "Epoch [263/1000], Loss: 25.9174\n",
      "Epoch [264/1000], Loss: 26.3436\n",
      "Epoch [265/1000], Loss: 34.8602\n",
      "Epoch [266/1000], Loss: 36.5718\n",
      "Epoch [267/1000], Loss: 32.5767\n",
      "Epoch [268/1000], Loss: 31.2108\n",
      "Epoch [269/1000], Loss: 25.3839\n",
      "Epoch [270/1000], Loss: 25.4913\n",
      "Epoch [271/1000], Loss: 25.2395\n",
      "Epoch [272/1000], Loss: 23.5546\n",
      "Epoch [273/1000], Loss: 27.4987\n",
      "Epoch [274/1000], Loss: 25.2701\n",
      "Epoch [275/1000], Loss: 24.4203\n",
      "Epoch [276/1000], Loss: 23.9389\n",
      "Epoch [277/1000], Loss: 24.4341\n",
      "Epoch [278/1000], Loss: 27.3723\n",
      "Epoch [279/1000], Loss: 26.2583\n",
      "Epoch [280/1000], Loss: 23.0673\n",
      "Epoch [281/1000], Loss: 22.3942\n",
      "Epoch [282/1000], Loss: 24.0977\n",
      "Epoch [283/1000], Loss: 21.5459\n",
      "Epoch [284/1000], Loss: 22.8542\n",
      "Epoch [285/1000], Loss: 27.0754\n",
      "Epoch [286/1000], Loss: 48.0546\n",
      "Epoch [287/1000], Loss: 39.9792\n",
      "Epoch [288/1000], Loss: 40.2493\n",
      "Epoch [289/1000], Loss: 37.2684\n",
      "Epoch [290/1000], Loss: 37.6576\n",
      "Epoch [291/1000], Loss: 27.4219\n",
      "Epoch [292/1000], Loss: 28.4909\n",
      "Epoch [293/1000], Loss: 24.5505\n",
      "Epoch [294/1000], Loss: 21.1057\n",
      "Epoch [295/1000], Loss: 20.4941\n",
      "Epoch [296/1000], Loss: 26.1501\n",
      "Epoch [297/1000], Loss: 22.6394\n",
      "Epoch [298/1000], Loss: 29.5254\n",
      "Epoch [299/1000], Loss: 24.7522\n",
      "Epoch [300/1000], Loss: 24.6737\n",
      "Epoch [301/1000], Loss: 25.1028\n",
      "Epoch [302/1000], Loss: 23.3162\n",
      "Epoch [303/1000], Loss: 22.1080\n",
      "Epoch [304/1000], Loss: 23.6702\n",
      "Epoch [305/1000], Loss: 23.7201\n",
      "Epoch [306/1000], Loss: 29.2422\n",
      "Epoch [307/1000], Loss: 26.7334\n",
      "Epoch [308/1000], Loss: 26.4897\n",
      "Epoch [309/1000], Loss: 26.1427\n",
      "Epoch [310/1000], Loss: 24.6335\n",
      "Epoch [311/1000], Loss: 39.1802\n",
      "Epoch [312/1000], Loss: 53.4611\n",
      "Epoch [313/1000], Loss: 86.6439\n",
      "Epoch [314/1000], Loss: 77.4284\n",
      "Epoch [315/1000], Loss: 46.1313\n",
      "Epoch [316/1000], Loss: 32.3544\n",
      "Epoch [317/1000], Loss: 30.9106\n",
      "Epoch [318/1000], Loss: 25.2698\n",
      "Epoch [319/1000], Loss: 26.5141\n",
      "Epoch [320/1000], Loss: 28.2137\n",
      "Epoch [321/1000], Loss: 20.3454\n",
      "Epoch [322/1000], Loss: 21.2652\n",
      "Epoch [323/1000], Loss: 20.6186\n",
      "Epoch [324/1000], Loss: 26.4503\n",
      "Epoch [325/1000], Loss: 28.9537\n",
      "Epoch [326/1000], Loss: 28.0770\n",
      "Epoch [327/1000], Loss: 21.8147\n",
      "Epoch [328/1000], Loss: 24.5396\n",
      "Epoch [329/1000], Loss: 24.4921\n",
      "Epoch [330/1000], Loss: 24.6408\n",
      "Epoch [331/1000], Loss: 27.6560\n",
      "Epoch [332/1000], Loss: 27.0426\n",
      "Epoch [333/1000], Loss: 21.9593\n",
      "Epoch [334/1000], Loss: 20.2170\n",
      "Epoch [335/1000], Loss: 18.7034\n",
      "Epoch [336/1000], Loss: 19.1068\n",
      "Epoch [337/1000], Loss: 20.9463\n",
      "Epoch [338/1000], Loss: 19.3923\n",
      "Epoch [339/1000], Loss: 20.6730\n",
      "Epoch [340/1000], Loss: 22.1892\n",
      "Epoch [341/1000], Loss: 23.5223\n",
      "Epoch [342/1000], Loss: 22.9199\n",
      "Epoch [343/1000], Loss: 21.5412\n",
      "Epoch [344/1000], Loss: 23.7716\n",
      "Epoch [345/1000], Loss: 22.5095\n",
      "Epoch [346/1000], Loss: 23.4223\n",
      "Epoch [347/1000], Loss: 20.2735\n",
      "Epoch [348/1000], Loss: 20.0113\n",
      "Epoch [349/1000], Loss: 18.8658\n",
      "Epoch [350/1000], Loss: 22.0448\n",
      "Epoch [351/1000], Loss: 22.6244\n",
      "Epoch [352/1000], Loss: 34.8970\n",
      "Epoch [353/1000], Loss: 23.5158\n",
      "Epoch [354/1000], Loss: 41.4982\n",
      "Epoch [355/1000], Loss: 22.0888\n",
      "Epoch [356/1000], Loss: 23.5827\n",
      "Epoch [357/1000], Loss: 21.4765\n",
      "Epoch [358/1000], Loss: 23.8682\n",
      "Epoch [359/1000], Loss: 22.9240\n",
      "Epoch [360/1000], Loss: 27.8991\n",
      "Epoch [361/1000], Loss: 23.6267\n",
      "Epoch [362/1000], Loss: 22.4285\n",
      "Epoch [363/1000], Loss: 23.6239\n",
      "Epoch [364/1000], Loss: 22.9230\n",
      "Epoch [365/1000], Loss: 25.8034\n",
      "Epoch [366/1000], Loss: 53.3200\n",
      "Epoch [367/1000], Loss: 28.3387\n",
      "Epoch [368/1000], Loss: 29.3437\n",
      "Epoch [369/1000], Loss: 29.4622\n",
      "Epoch [370/1000], Loss: 30.9270\n",
      "Epoch [371/1000], Loss: 27.7660\n",
      "Epoch [372/1000], Loss: 24.6975\n",
      "Epoch [373/1000], Loss: 22.1317\n",
      "Epoch [374/1000], Loss: 22.2838\n",
      "Epoch [375/1000], Loss: 18.4338\n",
      "Epoch [376/1000], Loss: 17.4025\n",
      "Epoch [377/1000], Loss: 18.2941\n",
      "Epoch [378/1000], Loss: 19.4828\n",
      "Epoch [379/1000], Loss: 17.0247\n",
      "Epoch [380/1000], Loss: 18.5088\n",
      "Epoch [381/1000], Loss: 17.8444\n",
      "Epoch [382/1000], Loss: 19.0392\n",
      "Epoch [383/1000], Loss: 20.2899\n",
      "Epoch [384/1000], Loss: 16.1689\n",
      "Epoch [385/1000], Loss: 24.9158\n",
      "Epoch [386/1000], Loss: 18.5009\n",
      "Epoch [387/1000], Loss: 19.9250\n",
      "Epoch [388/1000], Loss: 22.9079\n",
      "Epoch [389/1000], Loss: 23.1951\n",
      "Epoch [390/1000], Loss: 17.5314\n",
      "Epoch [391/1000], Loss: 18.8607\n",
      "Epoch [392/1000], Loss: 16.1708\n",
      "Epoch [393/1000], Loss: 30.3071\n",
      "Epoch [394/1000], Loss: 29.3321\n",
      "Epoch [395/1000], Loss: 25.3635\n",
      "Epoch [396/1000], Loss: 29.4041\n",
      "Epoch [397/1000], Loss: 20.0627\n",
      "Epoch [398/1000], Loss: 19.6501\n",
      "Epoch [399/1000], Loss: 17.5161\n",
      "Epoch [400/1000], Loss: 22.8473\n",
      "Epoch [401/1000], Loss: 16.2488\n",
      "Epoch [402/1000], Loss: 16.8270\n",
      "Epoch [403/1000], Loss: 16.0374\n",
      "Epoch [404/1000], Loss: 15.3966\n",
      "Epoch [405/1000], Loss: 20.5039\n",
      "Epoch [406/1000], Loss: 17.4756\n",
      "Epoch [407/1000], Loss: 16.2695\n",
      "Epoch [408/1000], Loss: 16.8625\n",
      "Epoch [409/1000], Loss: 15.3147\n",
      "Epoch [410/1000], Loss: 16.3985\n",
      "Epoch [411/1000], Loss: 21.1574\n",
      "Epoch [412/1000], Loss: 16.3390\n",
      "Epoch [413/1000], Loss: 26.0355\n",
      "Epoch [414/1000], Loss: 25.2008\n",
      "Epoch [415/1000], Loss: 26.5626\n",
      "Epoch [416/1000], Loss: 26.2020\n",
      "Epoch [417/1000], Loss: 24.1752\n",
      "Epoch [418/1000], Loss: 24.5237\n",
      "Epoch [419/1000], Loss: 27.2349\n",
      "Epoch [420/1000], Loss: 18.2884\n",
      "Epoch [421/1000], Loss: 23.0887\n",
      "Epoch [422/1000], Loss: 19.2432\n",
      "Epoch [423/1000], Loss: 22.0185\n",
      "Epoch [424/1000], Loss: 19.9879\n",
      "Epoch [425/1000], Loss: 17.5679\n",
      "Epoch [426/1000], Loss: 20.9748\n",
      "Epoch [427/1000], Loss: 21.6237\n",
      "Epoch [428/1000], Loss: 15.6069\n",
      "Epoch [429/1000], Loss: 19.7426\n",
      "Epoch [430/1000], Loss: 16.6039\n",
      "Epoch [431/1000], Loss: 19.1986\n",
      "Epoch [432/1000], Loss: 14.1681\n",
      "Epoch [433/1000], Loss: 16.9914\n",
      "Epoch [434/1000], Loss: 16.1772\n",
      "Epoch [435/1000], Loss: 17.2723\n",
      "Epoch [436/1000], Loss: 14.8176\n",
      "Epoch [437/1000], Loss: 12.8291\n",
      "Epoch [438/1000], Loss: 12.8849\n",
      "Epoch [439/1000], Loss: 15.4148\n",
      "Epoch [440/1000], Loss: 11.7082\n",
      "Epoch [441/1000], Loss: 16.1708\n",
      "Epoch [442/1000], Loss: 18.5460\n",
      "Epoch [443/1000], Loss: 17.2516\n",
      "Epoch [444/1000], Loss: 17.4346\n",
      "Epoch [445/1000], Loss: 18.2909\n",
      "Epoch [446/1000], Loss: 17.2261\n",
      "Epoch [447/1000], Loss: 17.8246\n",
      "Epoch [448/1000], Loss: 22.3001\n",
      "Epoch [449/1000], Loss: 22.8835\n",
      "Epoch [450/1000], Loss: 16.6382\n",
      "Epoch [451/1000], Loss: 21.0336\n",
      "Epoch [452/1000], Loss: 21.7967\n",
      "Epoch [453/1000], Loss: 29.3280\n",
      "Epoch [454/1000], Loss: 23.5781\n",
      "Epoch [455/1000], Loss: 21.7269\n",
      "Epoch [456/1000], Loss: 18.8695\n",
      "Epoch [457/1000], Loss: 15.7985\n",
      "Epoch [458/1000], Loss: 12.7317\n",
      "Epoch [459/1000], Loss: 12.5957\n",
      "Epoch [460/1000], Loss: 16.2628\n",
      "Epoch [461/1000], Loss: 12.4873\n",
      "Epoch [462/1000], Loss: 13.0579\n",
      "Epoch [463/1000], Loss: 11.5176\n",
      "Epoch [464/1000], Loss: 13.5277\n",
      "Epoch [465/1000], Loss: 15.3352\n",
      "Epoch [466/1000], Loss: 19.2635\n",
      "Epoch [467/1000], Loss: 12.7608\n",
      "Epoch [468/1000], Loss: 17.8277\n",
      "Epoch [469/1000], Loss: 17.4164\n",
      "Epoch [470/1000], Loss: 27.6251\n",
      "Epoch [471/1000], Loss: 20.1594\n",
      "Epoch [472/1000], Loss: 34.6235\n",
      "Epoch [473/1000], Loss: 23.0539\n",
      "Epoch [474/1000], Loss: 23.0195\n",
      "Epoch [475/1000], Loss: 21.8800\n",
      "Epoch [476/1000], Loss: 26.4171\n",
      "Epoch [477/1000], Loss: 28.0926\n",
      "Epoch [478/1000], Loss: 20.2645\n",
      "Epoch [479/1000], Loss: 17.6335\n",
      "Epoch [480/1000], Loss: 16.3899\n",
      "Epoch [481/1000], Loss: 14.8021\n",
      "Epoch [482/1000], Loss: 12.4158\n",
      "Epoch [483/1000], Loss: 12.6352\n",
      "Epoch [484/1000], Loss: 13.1364\n",
      "Epoch [485/1000], Loss: 14.9804\n",
      "Epoch [486/1000], Loss: 17.5766\n",
      "Epoch [487/1000], Loss: 14.0845\n",
      "Epoch [488/1000], Loss: 23.5264\n",
      "Epoch [489/1000], Loss: 16.0096\n",
      "Epoch [490/1000], Loss: 20.5424\n",
      "Epoch [491/1000], Loss: 25.9886\n",
      "Epoch [492/1000], Loss: 42.7605\n",
      "Epoch [493/1000], Loss: 39.8221\n",
      "Epoch [494/1000], Loss: 31.2262\n",
      "Epoch [495/1000], Loss: 25.6553\n",
      "Epoch [496/1000], Loss: 21.2691\n",
      "Epoch [497/1000], Loss: 17.2536\n",
      "Epoch [498/1000], Loss: 15.7393\n",
      "Epoch [499/1000], Loss: 13.0151\n",
      "Epoch [500/1000], Loss: 16.4240\n",
      "Epoch [501/1000], Loss: 12.4567\n",
      "Epoch [502/1000], Loss: 11.8413\n",
      "Epoch [503/1000], Loss: 11.2221\n",
      "Epoch [504/1000], Loss: 12.1935\n",
      "Epoch [505/1000], Loss: 11.3281\n",
      "Epoch [506/1000], Loss: 8.8788\n",
      "Epoch [507/1000], Loss: 24.4019\n",
      "Epoch [508/1000], Loss: 16.1604\n",
      "Epoch [509/1000], Loss: 22.1918\n",
      "Epoch [510/1000], Loss: 17.9775\n",
      "Epoch [511/1000], Loss: 12.5116\n",
      "Epoch [512/1000], Loss: 11.3613\n",
      "Epoch [513/1000], Loss: 13.5313\n",
      "Epoch [514/1000], Loss: 12.2594\n",
      "Epoch [515/1000], Loss: 12.9731\n",
      "Epoch [516/1000], Loss: 12.1528\n",
      "Epoch [517/1000], Loss: 11.1590\n",
      "Epoch [518/1000], Loss: 13.7546\n",
      "Epoch [519/1000], Loss: 11.0723\n",
      "Epoch [520/1000], Loss: 11.8281\n",
      "Epoch [521/1000], Loss: 11.2640\n",
      "Epoch [522/1000], Loss: 10.1710\n",
      "Epoch [523/1000], Loss: 9.5516\n",
      "Epoch [524/1000], Loss: 10.2821\n",
      "Epoch [525/1000], Loss: 9.2826\n",
      "Epoch [526/1000], Loss: 10.8746\n",
      "Epoch [527/1000], Loss: 12.2186\n",
      "Epoch [528/1000], Loss: 13.1952\n",
      "Epoch [529/1000], Loss: 14.2015\n",
      "Epoch [530/1000], Loss: 14.2518\n",
      "Epoch [531/1000], Loss: 28.5423\n",
      "Epoch [532/1000], Loss: 11.6387\n",
      "Epoch [533/1000], Loss: 16.8471\n",
      "Epoch [534/1000], Loss: 12.7761\n",
      "Epoch [535/1000], Loss: 11.4489\n",
      "Epoch [536/1000], Loss: 12.6711\n",
      "Epoch [537/1000], Loss: 17.1754\n",
      "Epoch [538/1000], Loss: 18.5851\n",
      "Epoch [539/1000], Loss: 12.8540\n",
      "Epoch [540/1000], Loss: 14.2725\n",
      "Epoch [541/1000], Loss: 12.8248\n",
      "Epoch [542/1000], Loss: 11.0859\n",
      "Epoch [543/1000], Loss: 12.3563\n",
      "Epoch [544/1000], Loss: 13.2619\n",
      "Epoch [545/1000], Loss: 13.8160\n",
      "Epoch [546/1000], Loss: 19.0902\n",
      "Epoch [547/1000], Loss: 18.6599\n",
      "Epoch [548/1000], Loss: 21.6096\n",
      "Epoch [549/1000], Loss: 24.5665\n",
      "Epoch [550/1000], Loss: 17.1743\n",
      "Epoch [551/1000], Loss: 19.0261\n",
      "Epoch [552/1000], Loss: 22.1466\n",
      "Epoch [553/1000], Loss: 15.0865\n",
      "Epoch [554/1000], Loss: 15.6447\n",
      "Epoch [555/1000], Loss: 14.7724\n",
      "Epoch [556/1000], Loss: 13.5385\n",
      "Epoch [557/1000], Loss: 10.2229\n",
      "Epoch [558/1000], Loss: 14.7584\n",
      "Epoch [559/1000], Loss: 17.1125\n",
      "Epoch [560/1000], Loss: 13.0699\n",
      "Epoch [561/1000], Loss: 11.8179\n",
      "Epoch [562/1000], Loss: 13.1788\n",
      "Epoch [563/1000], Loss: 14.5583\n",
      "Epoch [564/1000], Loss: 10.2261\n",
      "Epoch [565/1000], Loss: 11.6900\n",
      "Epoch [566/1000], Loss: 12.9206\n",
      "Epoch [567/1000], Loss: 12.5021\n",
      "Epoch [568/1000], Loss: 11.6108\n",
      "Epoch [569/1000], Loss: 13.3649\n",
      "Epoch [570/1000], Loss: 11.4278\n",
      "Epoch [571/1000], Loss: 12.4004\n",
      "Epoch [572/1000], Loss: 10.4581\n",
      "Epoch [573/1000], Loss: 13.9093\n",
      "Epoch [574/1000], Loss: 9.5580\n",
      "Epoch [575/1000], Loss: 12.8936\n",
      "Epoch [576/1000], Loss: 10.0263\n",
      "Epoch [577/1000], Loss: 9.5157\n",
      "Epoch [578/1000], Loss: 11.2040\n",
      "Epoch [579/1000], Loss: 8.6993\n",
      "Epoch [580/1000], Loss: 10.6806\n",
      "Epoch [581/1000], Loss: 9.3736\n",
      "Epoch [582/1000], Loss: 7.8463\n",
      "Epoch [583/1000], Loss: 7.7711\n",
      "Epoch [584/1000], Loss: 13.1838\n",
      "Epoch [585/1000], Loss: 12.3470\n",
      "Epoch [586/1000], Loss: 12.4585\n",
      "Epoch [587/1000], Loss: 13.7455\n",
      "Epoch [588/1000], Loss: 10.2915\n",
      "Epoch [589/1000], Loss: 8.5320\n",
      "Epoch [590/1000], Loss: 8.8529\n",
      "Epoch [591/1000], Loss: 7.4506\n",
      "Epoch [592/1000], Loss: 8.3613\n",
      "Epoch [593/1000], Loss: 7.1313\n",
      "Epoch [594/1000], Loss: 8.0959\n",
      "Epoch [595/1000], Loss: 7.9641\n",
      "Epoch [596/1000], Loss: 9.1450\n",
      "Epoch [597/1000], Loss: 12.6764\n",
      "Epoch [598/1000], Loss: 13.5977\n",
      "Epoch [599/1000], Loss: 8.5197\n",
      "Epoch [600/1000], Loss: 9.1570\n",
      "Epoch [601/1000], Loss: 8.5874\n",
      "Epoch [602/1000], Loss: 8.1630\n",
      "Epoch [603/1000], Loss: 12.8567\n",
      "Epoch [604/1000], Loss: 17.4282\n",
      "Epoch [605/1000], Loss: 14.4729\n",
      "Epoch [606/1000], Loss: 11.7931\n",
      "Epoch [607/1000], Loss: 11.5579\n",
      "Epoch [608/1000], Loss: 16.2305\n",
      "Epoch [609/1000], Loss: 20.6201\n",
      "Epoch [610/1000], Loss: 16.7659\n",
      "Epoch [611/1000], Loss: 26.9449\n",
      "Epoch [612/1000], Loss: 13.4793\n",
      "Epoch [613/1000], Loss: 18.9382\n",
      "Epoch [614/1000], Loss: 18.0310\n",
      "Epoch [615/1000], Loss: 18.1169\n",
      "Epoch [616/1000], Loss: 13.0342\n",
      "Epoch [617/1000], Loss: 12.2652\n",
      "Epoch [618/1000], Loss: 16.1644\n",
      "Epoch [619/1000], Loss: 11.3865\n",
      "Epoch [620/1000], Loss: 8.8134\n",
      "Epoch [621/1000], Loss: 11.0249\n",
      "Epoch [622/1000], Loss: 13.6682\n",
      "Epoch [623/1000], Loss: 8.1806\n",
      "Epoch [624/1000], Loss: 10.5792\n",
      "Epoch [625/1000], Loss: 9.6611\n",
      "Epoch [626/1000], Loss: 10.1179\n",
      "Epoch [627/1000], Loss: 8.8559\n",
      "Epoch [628/1000], Loss: 9.3794\n",
      "Epoch [629/1000], Loss: 10.1430\n",
      "Epoch [630/1000], Loss: 10.2305\n",
      "Epoch [631/1000], Loss: 11.5756\n",
      "Epoch [632/1000], Loss: 11.9222\n",
      "Epoch [633/1000], Loss: 12.2174\n",
      "Epoch [634/1000], Loss: 16.0727\n",
      "Epoch [635/1000], Loss: 19.2008\n",
      "Epoch [636/1000], Loss: 14.1639\n",
      "Epoch [637/1000], Loss: 13.0096\n",
      "Epoch [638/1000], Loss: 14.8855\n",
      "Epoch [639/1000], Loss: 18.2525\n",
      "Epoch [640/1000], Loss: 12.0506\n",
      "Epoch [641/1000], Loss: 11.7938\n",
      "Epoch [642/1000], Loss: 8.2343\n",
      "Epoch [643/1000], Loss: 9.1381\n",
      "Epoch [644/1000], Loss: 7.6951\n",
      "Epoch [645/1000], Loss: 7.9207\n",
      "Epoch [646/1000], Loss: 6.9495\n",
      "Epoch [647/1000], Loss: 6.5530\n",
      "Epoch [648/1000], Loss: 6.5560\n",
      "Epoch [649/1000], Loss: 5.3459\n",
      "Epoch [650/1000], Loss: 7.3443\n",
      "Epoch [651/1000], Loss: 6.6383\n",
      "Epoch [652/1000], Loss: 8.2164\n",
      "Epoch [653/1000], Loss: 7.3877\n",
      "Epoch [654/1000], Loss: 10.9934\n",
      "Epoch [655/1000], Loss: 12.2557\n",
      "Epoch [656/1000], Loss: 18.8480\n",
      "Epoch [657/1000], Loss: 28.7711\n",
      "Epoch [658/1000], Loss: 31.8303\n",
      "Epoch [659/1000], Loss: 35.0194\n",
      "Epoch [660/1000], Loss: 19.2325\n",
      "Epoch [661/1000], Loss: 11.1550\n",
      "Epoch [662/1000], Loss: 12.0396\n",
      "Epoch [663/1000], Loss: 8.2346\n",
      "Epoch [664/1000], Loss: 13.4531\n",
      "Epoch [665/1000], Loss: 14.6244\n",
      "Epoch [666/1000], Loss: 15.8390\n",
      "Epoch [667/1000], Loss: 14.5323\n",
      "Epoch [668/1000], Loss: 11.2677\n",
      "Epoch [669/1000], Loss: 10.5984\n",
      "Epoch [670/1000], Loss: 20.6127\n",
      "Epoch [671/1000], Loss: 10.7560\n",
      "Epoch [672/1000], Loss: 9.5336\n",
      "Epoch [673/1000], Loss: 12.3419\n",
      "Epoch [674/1000], Loss: 10.7882\n",
      "Epoch [675/1000], Loss: 8.4786\n",
      "Epoch [676/1000], Loss: 6.6297\n",
      "Epoch [677/1000], Loss: 5.5644\n",
      "Epoch [678/1000], Loss: 6.7238\n",
      "Epoch [679/1000], Loss: 10.4801\n",
      "Epoch [680/1000], Loss: 12.9643\n",
      "Epoch [681/1000], Loss: 8.9348\n",
      "Epoch [682/1000], Loss: 8.9316\n",
      "Epoch [683/1000], Loss: 7.7756\n",
      "Epoch [684/1000], Loss: 5.8870\n",
      "Epoch [685/1000], Loss: 5.2716\n",
      "Epoch [686/1000], Loss: 8.6001\n",
      "Epoch [687/1000], Loss: 24.0665\n",
      "Epoch [688/1000], Loss: 19.8772\n",
      "Epoch [689/1000], Loss: 46.1863\n",
      "Epoch [690/1000], Loss: 448.3904\n",
      "Epoch [691/1000], Loss: 162.5733\n",
      "Epoch [692/1000], Loss: 97.5620\n",
      "Epoch [693/1000], Loss: 78.9099\n",
      "Epoch [694/1000], Loss: 56.6760\n",
      "Epoch [695/1000], Loss: 38.5191\n",
      "Epoch [696/1000], Loss: 32.0516\n",
      "Epoch [697/1000], Loss: 27.0505\n",
      "Epoch [698/1000], Loss: 20.2241\n",
      "Epoch [699/1000], Loss: 17.7402\n",
      "Epoch [700/1000], Loss: 16.3178\n",
      "Epoch [701/1000], Loss: 14.7407\n",
      "Epoch [702/1000], Loss: 13.9953\n",
      "Epoch [703/1000], Loss: 15.0597\n",
      "Epoch [704/1000], Loss: 16.8847\n",
      "Epoch [705/1000], Loss: 14.5717\n",
      "Epoch [706/1000], Loss: 11.2999\n",
      "Epoch [707/1000], Loss: 11.0095\n",
      "Epoch [708/1000], Loss: 12.1126\n",
      "Epoch [709/1000], Loss: 12.7914\n",
      "Epoch [710/1000], Loss: 9.0658\n",
      "Epoch [711/1000], Loss: 9.5293\n",
      "Epoch [712/1000], Loss: 10.4324\n",
      "Epoch [713/1000], Loss: 9.4452\n",
      "Epoch [714/1000], Loss: 9.8520\n",
      "Epoch [715/1000], Loss: 8.3876\n",
      "Epoch [716/1000], Loss: 9.5568\n",
      "Epoch [717/1000], Loss: 10.7780\n",
      "Epoch [718/1000], Loss: 8.7799\n",
      "Epoch [719/1000], Loss: 9.1107\n",
      "Epoch [720/1000], Loss: 7.7381\n",
      "Epoch [721/1000], Loss: 8.3944\n",
      "Epoch [722/1000], Loss: 7.7728\n",
      "Epoch [723/1000], Loss: 7.7081\n",
      "Epoch [724/1000], Loss: 8.4731\n",
      "Epoch [725/1000], Loss: 9.0418\n",
      "Epoch [726/1000], Loss: 7.1874\n",
      "Epoch [727/1000], Loss: 7.5315\n",
      "Epoch [728/1000], Loss: 6.5469\n",
      "Epoch [729/1000], Loss: 8.2428\n",
      "Epoch [730/1000], Loss: 7.8831\n",
      "Epoch [731/1000], Loss: 6.8628\n",
      "Epoch [732/1000], Loss: 6.5187\n",
      "Epoch [733/1000], Loss: 6.8407\n",
      "Epoch [734/1000], Loss: 7.2035\n",
      "Epoch [735/1000], Loss: 8.6968\n",
      "Epoch [736/1000], Loss: 7.6753\n",
      "Epoch [737/1000], Loss: 6.8521\n",
      "Epoch [738/1000], Loss: 8.9082\n",
      "Epoch [739/1000], Loss: 8.0569\n",
      "Epoch [740/1000], Loss: 7.2900\n",
      "Epoch [741/1000], Loss: 6.4605\n",
      "Epoch [742/1000], Loss: 7.7287\n",
      "Epoch [743/1000], Loss: 5.7964\n",
      "Epoch [744/1000], Loss: 6.9860\n",
      "Epoch [745/1000], Loss: 7.5280\n",
      "Epoch [746/1000], Loss: 9.0263\n",
      "Epoch [747/1000], Loss: 12.5011\n",
      "Epoch [748/1000], Loss: 9.0352\n",
      "Epoch [749/1000], Loss: 8.8918\n",
      "Epoch [750/1000], Loss: 10.7341\n",
      "Epoch [751/1000], Loss: 9.2862\n",
      "Epoch [752/1000], Loss: 10.1070\n",
      "Epoch [753/1000], Loss: 10.2409\n",
      "Epoch [754/1000], Loss: 8.7572\n",
      "Epoch [755/1000], Loss: 5.8958\n",
      "Epoch [756/1000], Loss: 6.0519\n",
      "Epoch [757/1000], Loss: 6.3140\n",
      "Epoch [758/1000], Loss: 10.8452\n",
      "Epoch [759/1000], Loss: 9.3961\n",
      "Epoch [760/1000], Loss: 7.0932\n",
      "Epoch [761/1000], Loss: 5.7032\n",
      "Epoch [762/1000], Loss: 12.4967\n",
      "Epoch [763/1000], Loss: 9.8190\n",
      "Epoch [764/1000], Loss: 6.8338\n",
      "Epoch [765/1000], Loss: 7.6831\n",
      "Epoch [766/1000], Loss: 7.9154\n",
      "Epoch [767/1000], Loss: 7.6462\n",
      "Epoch [768/1000], Loss: 8.2408\n",
      "Epoch [769/1000], Loss: 10.4493\n",
      "Epoch [770/1000], Loss: 5.8506\n",
      "Epoch [771/1000], Loss: 10.2104\n",
      "Epoch [772/1000], Loss: 8.2556\n",
      "Epoch [773/1000], Loss: 5.8998\n",
      "Epoch [774/1000], Loss: 5.9240\n",
      "Epoch [775/1000], Loss: 7.9354\n",
      "Epoch [776/1000], Loss: 7.2106\n",
      "Epoch [777/1000], Loss: 8.2478\n",
      "Epoch [778/1000], Loss: 5.7714\n",
      "Epoch [779/1000], Loss: 4.8653\n",
      "Epoch [780/1000], Loss: 8.9456\n",
      "Epoch [781/1000], Loss: 7.2336\n",
      "Epoch [782/1000], Loss: 9.2654\n",
      "Epoch [783/1000], Loss: 6.3846\n",
      "Epoch [784/1000], Loss: 9.7746\n",
      "Epoch [785/1000], Loss: 8.8022\n",
      "Epoch [786/1000], Loss: 8.1172\n",
      "Epoch [787/1000], Loss: 8.6108\n",
      "Epoch [788/1000], Loss: 6.0210\n",
      "Epoch [789/1000], Loss: 7.9428\n",
      "Epoch [790/1000], Loss: 9.2374\n",
      "Epoch [791/1000], Loss: 5.4983\n",
      "Epoch [792/1000], Loss: 8.0305\n",
      "Epoch [793/1000], Loss: 4.5990\n",
      "Epoch [794/1000], Loss: 8.5273\n",
      "Epoch [795/1000], Loss: 8.5458\n",
      "Epoch [796/1000], Loss: 11.6387\n",
      "Epoch [797/1000], Loss: 6.7709\n",
      "Epoch [798/1000], Loss: 16.6396\n",
      "Epoch [799/1000], Loss: 8.4453\n",
      "Epoch [800/1000], Loss: 6.0469\n",
      "Epoch [801/1000], Loss: 4.5584\n",
      "Epoch [802/1000], Loss: 10.0396\n",
      "Epoch [803/1000], Loss: 8.6686\n",
      "Epoch [804/1000], Loss: 6.0606\n",
      "Epoch [805/1000], Loss: 6.5634\n",
      "Epoch [806/1000], Loss: 9.4142\n",
      "Epoch [807/1000], Loss: 8.4221\n",
      "Epoch [808/1000], Loss: 8.9860\n",
      "Epoch [809/1000], Loss: 12.7096\n",
      "Epoch [810/1000], Loss: 11.8506\n",
      "Epoch [811/1000], Loss: 15.6267\n",
      "Epoch [812/1000], Loss: 9.6867\n",
      "Epoch [813/1000], Loss: 6.7472\n",
      "Epoch [814/1000], Loss: 6.3341\n",
      "Epoch [815/1000], Loss: 7.8398\n",
      "Epoch [816/1000], Loss: 9.8370\n",
      "Epoch [817/1000], Loss: 6.1430\n",
      "Epoch [818/1000], Loss: 5.5024\n",
      "Epoch [819/1000], Loss: 4.6405\n",
      "Epoch [820/1000], Loss: 6.4335\n",
      "Epoch [821/1000], Loss: 10.5902\n",
      "Epoch [822/1000], Loss: 5.6172\n",
      "Epoch [823/1000], Loss: 6.4910\n",
      "Epoch [824/1000], Loss: 5.6430\n",
      "Epoch [825/1000], Loss: 6.3602\n",
      "Epoch [826/1000], Loss: 4.4678\n",
      "Epoch [827/1000], Loss: 8.3640\n",
      "Epoch [828/1000], Loss: 9.0638\n",
      "Epoch [829/1000], Loss: 5.1177\n",
      "Epoch [830/1000], Loss: 4.4157\n",
      "Epoch [831/1000], Loss: 5.8345\n",
      "Epoch [832/1000], Loss: 5.3925\n",
      "Epoch [833/1000], Loss: 4.5310\n",
      "Epoch [834/1000], Loss: 5.7281\n",
      "Epoch [835/1000], Loss: 6.5877\n",
      "Epoch [836/1000], Loss: 6.4693\n",
      "Epoch [837/1000], Loss: 14.2288\n",
      "Epoch [838/1000], Loss: 13.6332\n",
      "Epoch [839/1000], Loss: 9.9719\n",
      "Epoch [840/1000], Loss: 6.1442\n",
      "Epoch [841/1000], Loss: 10.3850\n",
      "Epoch [842/1000], Loss: 13.8165\n",
      "Epoch [843/1000], Loss: 21.9251\n",
      "Epoch [844/1000], Loss: 23.8500\n",
      "Epoch [845/1000], Loss: 17.9823\n",
      "Epoch [846/1000], Loss: 11.9673\n",
      "Epoch [847/1000], Loss: 13.2506\n",
      "Epoch [848/1000], Loss: 7.8841\n",
      "Epoch [849/1000], Loss: 5.0394\n",
      "Epoch [850/1000], Loss: 10.0361\n",
      "Epoch [851/1000], Loss: 6.7886\n",
      "Epoch [852/1000], Loss: 4.3681\n",
      "Epoch [853/1000], Loss: 5.6943\n",
      "Epoch [854/1000], Loss: 9.2356\n",
      "Epoch [855/1000], Loss: 5.0549\n",
      "Epoch [856/1000], Loss: 7.0037\n",
      "Epoch [857/1000], Loss: 6.8566\n",
      "Epoch [858/1000], Loss: 9.3325\n",
      "Epoch [859/1000], Loss: 5.5121\n",
      "Epoch [860/1000], Loss: 4.6279\n",
      "Epoch [861/1000], Loss: 3.6514\n",
      "Epoch [862/1000], Loss: 4.7496\n",
      "Epoch [863/1000], Loss: 3.4448\n",
      "Epoch [864/1000], Loss: 3.7458\n",
      "Epoch [865/1000], Loss: 6.2035\n",
      "Epoch [866/1000], Loss: 10.4102\n",
      "Epoch [867/1000], Loss: 8.0256\n",
      "Epoch [868/1000], Loss: 11.4633\n",
      "Epoch [869/1000], Loss: 6.8784\n",
      "Epoch [870/1000], Loss: 6.5867\n",
      "Epoch [871/1000], Loss: 5.7902\n",
      "Epoch [872/1000], Loss: 4.9947\n",
      "Epoch [873/1000], Loss: 6.7555\n",
      "Epoch [874/1000], Loss: 4.8055\n",
      "Epoch [875/1000], Loss: 5.3356\n",
      "Epoch [876/1000], Loss: 4.7985\n",
      "Epoch [877/1000], Loss: 5.9807\n",
      "Epoch [878/1000], Loss: 6.7589\n",
      "Epoch [879/1000], Loss: 6.5525\n",
      "Epoch [880/1000], Loss: 4.4662\n",
      "Epoch [881/1000], Loss: 5.4068\n",
      "Epoch [882/1000], Loss: 6.9825\n",
      "Epoch [883/1000], Loss: 9.8715\n",
      "Epoch [884/1000], Loss: 5.0326\n",
      "Epoch [885/1000], Loss: 5.4175\n",
      "Epoch [886/1000], Loss: 5.0508\n",
      "Epoch [887/1000], Loss: 7.9639\n",
      "Epoch [888/1000], Loss: 5.3085\n",
      "Epoch [889/1000], Loss: 6.2888\n",
      "Epoch [890/1000], Loss: 4.2747\n",
      "Epoch [891/1000], Loss: 4.9551\n",
      "Epoch [892/1000], Loss: 4.0945\n",
      "Epoch [893/1000], Loss: 4.7156\n",
      "Epoch [894/1000], Loss: 4.2182\n",
      "Epoch [895/1000], Loss: 4.2659\n",
      "Epoch [896/1000], Loss: 5.1359\n",
      "Epoch [897/1000], Loss: 6.1224\n",
      "Epoch [898/1000], Loss: 8.7152\n",
      "Epoch [899/1000], Loss: 9.2038\n",
      "Epoch [900/1000], Loss: 8.6647\n",
      "Epoch [901/1000], Loss: 15.2134\n",
      "Epoch [902/1000], Loss: 14.1340\n",
      "Epoch [903/1000], Loss: 8.7858\n",
      "Epoch [904/1000], Loss: 16.1714\n",
      "Epoch [905/1000], Loss: 28.2784\n",
      "Epoch [906/1000], Loss: 17.3288\n",
      "Epoch [907/1000], Loss: 9.5123\n",
      "Epoch [908/1000], Loss: 13.5085\n",
      "Epoch [909/1000], Loss: 12.2315\n",
      "Epoch [910/1000], Loss: 21.6702\n",
      "Epoch [911/1000], Loss: 10.0625\n",
      "Epoch [912/1000], Loss: 8.8651\n",
      "Epoch [913/1000], Loss: 16.1038\n",
      "Epoch [914/1000], Loss: 9.8309\n",
      "Epoch [915/1000], Loss: 9.2986\n",
      "Epoch [916/1000], Loss: 6.0418\n",
      "Epoch [917/1000], Loss: 9.8938\n",
      "Epoch [918/1000], Loss: 4.9000\n",
      "Epoch [919/1000], Loss: 6.2776\n",
      "Epoch [920/1000], Loss: 5.7399\n",
      "Epoch [921/1000], Loss: 6.9198\n",
      "Epoch [922/1000], Loss: 3.9952\n",
      "Epoch [923/1000], Loss: 3.7117\n",
      "Epoch [924/1000], Loss: 5.9153\n",
      "Epoch [925/1000], Loss: 4.7549\n",
      "Epoch [926/1000], Loss: 3.5551\n",
      "Epoch [927/1000], Loss: 3.1659\n",
      "Epoch [928/1000], Loss: 4.4207\n",
      "Epoch [929/1000], Loss: 2.8404\n",
      "Epoch [930/1000], Loss: 3.7117\n",
      "Epoch [931/1000], Loss: 3.8883\n",
      "Epoch [932/1000], Loss: 2.9307\n",
      "Epoch [933/1000], Loss: 3.8146\n",
      "Epoch [934/1000], Loss: 3.6047\n",
      "Epoch [935/1000], Loss: 4.5914\n",
      "Epoch [936/1000], Loss: 22.8886\n",
      "Epoch [937/1000], Loss: 13.4102\n",
      "Epoch [938/1000], Loss: 13.8447\n",
      "Epoch [939/1000], Loss: 6.2771\n",
      "Epoch [940/1000], Loss: 8.4490\n",
      "Epoch [941/1000], Loss: 9.2852\n",
      "Epoch [942/1000], Loss: 5.2353\n",
      "Epoch [943/1000], Loss: 5.8876\n",
      "Epoch [944/1000], Loss: 6.8561\n",
      "Epoch [945/1000], Loss: 5.0095\n",
      "Epoch [946/1000], Loss: 3.9873\n",
      "Epoch [947/1000], Loss: 4.7710\n",
      "Epoch [948/1000], Loss: 3.5950\n",
      "Epoch [949/1000], Loss: 4.6042\n",
      "Epoch [950/1000], Loss: 4.7167\n",
      "Epoch [951/1000], Loss: 3.7817\n",
      "Epoch [952/1000], Loss: 3.0373\n",
      "Epoch [953/1000], Loss: 4.3350\n",
      "Epoch [954/1000], Loss: 4.4198\n",
      "Epoch [955/1000], Loss: 3.5141\n",
      "Epoch [956/1000], Loss: 4.1586\n",
      "Epoch [957/1000], Loss: 3.2658\n",
      "Epoch [958/1000], Loss: 6.9216\n",
      "Epoch [959/1000], Loss: 4.5038\n",
      "Epoch [960/1000], Loss: 4.1523\n",
      "Epoch [961/1000], Loss: 3.3947\n",
      "Epoch [962/1000], Loss: 2.8106\n",
      "Epoch [963/1000], Loss: 4.4504\n",
      "Epoch [964/1000], Loss: 2.9227\n",
      "Epoch [965/1000], Loss: 4.4587\n",
      "Epoch [966/1000], Loss: 2.3448\n",
      "Epoch [967/1000], Loss: 4.3348\n",
      "Epoch [968/1000], Loss: 2.6401\n",
      "Epoch [969/1000], Loss: 4.2777\n",
      "Epoch [970/1000], Loss: 4.3510\n",
      "Epoch [971/1000], Loss: 2.8403\n",
      "Epoch [972/1000], Loss: 3.4164\n",
      "Epoch [973/1000], Loss: 7.2827\n",
      "Epoch [974/1000], Loss: 19.5832\n",
      "Epoch [975/1000], Loss: 13.2204\n",
      "Epoch [976/1000], Loss: 20.9373\n",
      "Epoch [977/1000], Loss: 7.1370\n",
      "Epoch [978/1000], Loss: 30.1815\n",
      "Epoch [979/1000], Loss: 35.4262\n",
      "Epoch [980/1000], Loss: 10.6222\n",
      "Epoch [981/1000], Loss: 10.9312\n",
      "Epoch [982/1000], Loss: 12.7267\n",
      "Epoch [983/1000], Loss: 7.3069\n",
      "Epoch [984/1000], Loss: 3.9838\n",
      "Epoch [985/1000], Loss: 3.3499\n",
      "Epoch [986/1000], Loss: 3.5312\n",
      "Epoch [987/1000], Loss: 4.7023\n",
      "Epoch [988/1000], Loss: 3.1902\n",
      "Epoch [989/1000], Loss: 3.8078\n",
      "Epoch [990/1000], Loss: 3.0561\n",
      "Epoch [991/1000], Loss: 3.8417\n",
      "Epoch [992/1000], Loss: 6.3146\n",
      "Epoch [993/1000], Loss: 3.7801\n",
      "Epoch [994/1000], Loss: 14.3265\n",
      "Epoch [995/1000], Loss: 6.5178\n",
      "Epoch [996/1000], Loss: 3.1029\n",
      "Epoch [997/1000], Loss: 6.2246\n",
      "Epoch [998/1000], Loss: 7.1750\n",
      "Epoch [999/1000], Loss: 10.0137\n",
      "Epoch [1000/1000], Loss: 8.2490\n"
     ]
    }
   ],
   "source": [
    "loss, classifier = train(classifier=classifier, dataloader=dataloader, criterion=nn.CrossEntropyLoss(reduction=\"sum\"), optimizer=torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mloss\u001b[49m)           \n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLine Chart\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m)     \n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(loss)           \n",
    "plt.title(\"Line Chart\")  \n",
    "plt.xlabel(\"epoch\")     \n",
    "plt.ylabel(\"loss\")      \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: 0\n",
      "Etiqueta real: 0\n"
     ]
    }
   ],
   "source": [
    "def prediction(image):\n",
    "    # Envía la imagen a GPU, obtiene logits y selecciona la clase predicha.\n",
    "    output = classifier(image.to(\"cuda\"))\n",
    "    pred_class = torch.argmax(output, dim=1)\n",
    "    return pred_class\n",
    "\n",
    "# Seleccionar una imagen aleatoria del conjunto de prueba\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "image, label = test_set[idx]\n",
    "\n",
    "# Evaluar sin calcular gradientes\n",
    "with torch.no_grad():\n",
    "    pred = prediction(image)\n",
    "\n",
    "# Convertir etiqueta one-hot a índice si es necesario\n",
    "if label.numel() > 1:\n",
    "    true_class = torch.argmax(label)\n",
    "else:\n",
    "    true_class = label\n",
    "\n",
    "print(\"Predicción:\", pred.item())\n",
    "print(\"Etiqueta real:\", true_class.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.8818764568764569\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Listas para almacenar etiquetas verdaderas y predichas\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Colocar el modelo en modo evaluación\n",
    "classifier.eval()\n",
    "\n",
    "# Desactivar cálculo de gradientes durante la evaluación\n",
    "with torch.no_grad():\n",
    "    for image, label in test_set:\n",
    "        # Obtener la predicción (se asume que 'prediction' ya mueve la imagen a GPU)\n",
    "        pred = prediction(image)\n",
    "        \n",
    "        # Convertir la etiqueta one-hot a índice, si es necesario\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label_idx = torch.argmax(label).item() if label.numel() > 1 else label.item()\n",
    "        else:\n",
    "            label_idx = label\n",
    "        \n",
    "        true_labels.append(label_idx)\n",
    "        pred_labels.append(pred.item())\n",
    "\n",
    "# Calcular la precisión (macro average)\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "print(\"Precisión del modelo:\", precision)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
