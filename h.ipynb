{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n",
    "import optuna\n",
    "from joblib import Parallel, delayed\n",
    "from utils.Loader import NEUDataset\n",
    "from utils.Perspectiver import Perspectiver\n",
    "from source.Prototype1 import Prototype1\n",
    "from source.Classifier import MetalClassifier\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "import multiprocessing\n",
    "import torch\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.restoration import denoise_wavelet\n",
    "import random\n",
    "import math\n",
    "from PIL import Image\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from scipy.ndimage import maximum_filter, minimum_filter, label, generate_binary_structure\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from scipy.ndimage import label as ndi_label, binary_dilation\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda cores\n",
      "Dataset: train created!\n"
     ]
    }
   ],
   "source": [
    "model = Prototype1(num_attention_heads=16).to(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda cores\")\n",
    "    model.cuda()\n",
    "\n",
    "dataset = NEUDataset(set=\"train\", seed=555, scale=0.5, best_param=True, output_path=\"outputs_k10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0005\n",
    "EPOCH = 1000\n",
    "BATCH_SIZE = len(dataset)//2\n",
    "THREADS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "Best loss: '2532'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    loss_record = []\n",
    "    for epoch in range(EPOCH):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Desempaquetar ignorando 'labels'\n",
    "        for images, _, best_parameters in dataloader:\n",
    "            images = images.to('cuda')\n",
    "            best_parameters = best_parameters.to('cuda')\n",
    "\n",
    "            # Forward: el modelo predice sp y sr\n",
    "            pred_reg = model(images)  # salida de regresión\n",
    "\n",
    "            # Calcular la pérdida de regresión\n",
    "            loss = criterion(pred_reg, best_parameters)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        loss_record.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCH}], Loss: {avg_loss:.4f}\")\n",
    "    return loss_record, model\n",
    "\n",
    "loss, model = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parametros = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model parameters: {total_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)           \n",
    "plt.title(\"Line Chart\")  \n",
    "plt.xlabel(\"epoch\")     \n",
    "plt.ylabel(\"loss\")      \n",
    "plt.show()               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar solo la red neuronal en un archivo .pth\n",
    "torch.save(model.state_dict(), \"h2.pth\")\n",
    "print(\"Modelo guardado exitosamente en h2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = Prototype1(num_attention_heads=16)\n",
    "loaded_model.load_state_dict(torch.load(\"h2.pth\", map_location=torch.device('cpu')))\n",
    "loaded_model.to(\"cuda\")\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_barchartImage(image):\n",
    "    x = np.arange(image.shape[0])\n",
    "    y = np.arange(image.shape[1])\n",
    "    x, y = np.meshgrid(x, y)\n",
    "\n",
    "    # Flatten arrays for plotting\n",
    "    x = x.flatten()\n",
    "    y = y.flatten()\n",
    "    z = np.zeros_like(x)\n",
    "    dx = dy = np.ones_like(x)\n",
    "    dz = image.flatten()\n",
    "\n",
    "    # Plot the 3D bar chart\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.bar3d(x, y, z, dx, dy, dz, shade=True)\n",
    "\n",
    "    # Add labels and title\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Value')\n",
    "    ax.set_title('3D Bar Chart of (200, 200) Array')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def use_model_to_cluster(model: nn.Module, image: torch.tensor):\n",
    "    best_parameters = model(image.to(\"cuda\"))\n",
    "    print(best_parameters)\n",
    "    sp = best_parameters[0][0]\n",
    "    if sp <= 1: sp = 1\n",
    "    sr = best_parameters[0][1]\n",
    "    if sr <= 1: sr = 1\n",
    "    return sp, sr\n",
    "\n",
    "def plot_how_the_model_is_watching_the_picture(model: nn.Module, dataset: NEUDataset):\n",
    "    image, label = dataset.__getitem__(index= random.randint(0, len(dataset)))\n",
    "    print(label)\n",
    "    original_image = Perspectiver.grayscale_to_rgb(Perspectiver.normalize_to_uint8(image.detach().cpu().numpy()[0]))\n",
    "    sp , sr = use_model_to_cluster(loaded_model, image)\n",
    "    clustered_image = Perspectiver.meanShift(original_image, float(sp), float(sr))\n",
    "    Perspectiver.plotComparison(imageBefore=original_image, imageAfter=clustered_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: test created!\n"
     ]
    }
   ],
   "source": [
    "test_set = NEUDataset(set=\"test\", seed=55, scale=0.5, best_param=False)\n",
    "#plot_how_the_model_is_watching_the_picture(loaded_model, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: train created!\n",
      "1656\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.00005\n",
    "EPOCH = 1200\n",
    "BATCH_SIZE = len(dataset)\n",
    "THREADS = 24\n",
    "dataset = NEUDataset(set=\"train\", seed=555, scale=0.5)\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(classifier, dataloader, criterion, optimizer):\n",
    "    loss_record = []\n",
    "    classifier.train()\n",
    "    for epoch in range(EPOCH):\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Desempaquetar ignorando 'labels'\n",
    "        for images, label in dataloader:\n",
    "            images = images.to('cuda')\n",
    "            label = label[:, 0:6].float().to(device)\n",
    "\n",
    "            # Forward: el modelo predice sp y sr\n",
    "            pred_reg = classifier(images)  # salida de regresión\n",
    "\n",
    "            #print(pred_reg.shape)\n",
    "            #print(label.shape)\n",
    "\n",
    "            loss = criterion(pred_reg, label)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "\n",
    "            #Opcional: imprimir para depurar\n",
    "            #print(\"Logits:\", pred_reg)\n",
    "            #print(\"Labels:\", label)\n",
    "            #print(\"Batch Loss:\", loss.item())\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        loss_record.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch + 1}/{EPOCH}], Loss: {avg_loss:.4f}\")\n",
    "    return loss_record, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best:  num_feature_extractors=2, features_per_extractor=6 ---> loss 24.012\n",
    "Best:  num_feature_extractors=2, features_per_extractor=8 ---> loss 3.0934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda cores\n",
      "Model parameters: 1175448\n"
     ]
    }
   ],
   "source": [
    "classifier =  MetalClassifier(output_len=6, num_feature_extractors=2, features_per_extractor=8).to(\"cuda\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda cores\")\n",
    "    classifier.cuda()\n",
    "\n",
    "total_parametros = sum(p.numel() for p in classifier.parameters())\n",
    "print(f\"Model parameters: {total_parametros}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1200], Loss: 2967.3906\n",
      "Epoch [2/1200], Loss: 2967.3784\n",
      "Epoch [3/1200], Loss: 2967.3665\n",
      "Epoch [4/1200], Loss: 2967.3550\n",
      "Epoch [5/1200], Loss: 2967.3435\n",
      "Epoch [6/1200], Loss: 2967.3328\n",
      "Epoch [7/1200], Loss: 2967.3225\n",
      "Epoch [8/1200], Loss: 2967.3125\n",
      "Epoch [9/1200], Loss: 2967.3022\n",
      "Epoch [10/1200], Loss: 2967.2930\n",
      "Epoch [11/1200], Loss: 2967.2842\n",
      "Epoch [12/1200], Loss: 2967.2754\n",
      "Epoch [13/1200], Loss: 2967.2676\n",
      "Epoch [14/1200], Loss: 2967.2595\n",
      "Epoch [15/1200], Loss: 2967.2520\n",
      "Epoch [16/1200], Loss: 2967.2451\n",
      "Epoch [17/1200], Loss: 2967.2383\n",
      "Epoch [18/1200], Loss: 2967.2317\n",
      "Epoch [19/1200], Loss: 2967.2258\n",
      "Epoch [20/1200], Loss: 2967.2202\n",
      "Epoch [21/1200], Loss: 2967.2148\n",
      "Epoch [22/1200], Loss: 2967.2097\n",
      "Epoch [23/1200], Loss: 2967.2051\n",
      "Epoch [24/1200], Loss: 2967.2007\n",
      "Epoch [25/1200], Loss: 2967.1963\n",
      "Epoch [26/1200], Loss: 2967.1929\n",
      "Epoch [27/1200], Loss: 2967.1890\n",
      "Epoch [28/1200], Loss: 2967.1855\n",
      "Epoch [29/1200], Loss: 2967.1824\n",
      "Epoch [30/1200], Loss: 2967.1797\n",
      "Epoch [31/1200], Loss: 2967.1772\n",
      "Epoch [32/1200], Loss: 2967.1748\n",
      "Epoch [33/1200], Loss: 2967.1726\n",
      "Epoch [34/1200], Loss: 2967.1707\n",
      "Epoch [35/1200], Loss: 2967.1689\n",
      "Epoch [36/1200], Loss: 2967.1672\n",
      "Epoch [37/1200], Loss: 2967.1658\n",
      "Epoch [38/1200], Loss: 2967.1646\n",
      "Epoch [39/1200], Loss: 2967.1633\n",
      "Epoch [40/1200], Loss: 2967.1621\n",
      "Epoch [41/1200], Loss: 2967.1611\n",
      "Epoch [42/1200], Loss: 2967.1602\n",
      "Epoch [43/1200], Loss: 2967.1597\n",
      "Epoch [44/1200], Loss: 2967.1587\n",
      "Epoch [45/1200], Loss: 2967.1582\n",
      "Epoch [46/1200], Loss: 2967.1577\n",
      "Epoch [47/1200], Loss: 2967.1572\n",
      "Epoch [48/1200], Loss: 2967.1567\n",
      "Epoch [49/1200], Loss: 2967.1562\n",
      "Epoch [50/1200], Loss: 2967.1560\n",
      "Epoch [51/1200], Loss: 2967.1558\n",
      "Epoch [52/1200], Loss: 2967.1553\n",
      "Epoch [53/1200], Loss: 2967.1550\n",
      "Epoch [54/1200], Loss: 2967.1548\n",
      "Epoch [55/1200], Loss: 2967.1545\n",
      "Epoch [56/1200], Loss: 2967.1543\n",
      "Epoch [57/1200], Loss: 2967.1543\n",
      "Epoch [58/1200], Loss: 2967.1541\n",
      "Epoch [59/1200], Loss: 2967.1538\n",
      "Epoch [60/1200], Loss: 2967.1536\n",
      "Epoch [61/1200], Loss: 2967.1536\n",
      "Epoch [62/1200], Loss: 2967.1533\n",
      "Epoch [63/1200], Loss: 2967.1533\n",
      "Epoch [64/1200], Loss: 2967.1533\n",
      "Epoch [65/1200], Loss: 2967.1528\n",
      "Epoch [66/1200], Loss: 2967.1528\n",
      "Epoch [67/1200], Loss: 2967.1528\n",
      "Epoch [68/1200], Loss: 2967.1528\n",
      "Epoch [69/1200], Loss: 2967.1523\n",
      "Epoch [70/1200], Loss: 2967.1523\n",
      "Epoch [71/1200], Loss: 2967.1521\n",
      "Epoch [72/1200], Loss: 2967.1519\n",
      "Epoch [73/1200], Loss: 2967.1519\n",
      "Epoch [74/1200], Loss: 2967.1516\n",
      "Epoch [75/1200], Loss: 2967.1516\n",
      "Epoch [76/1200], Loss: 2967.1514\n",
      "Epoch [77/1200], Loss: 2967.1509\n",
      "Epoch [78/1200], Loss: 2967.1506\n",
      "Epoch [79/1200], Loss: 2967.1501\n",
      "Epoch [80/1200], Loss: 2967.1499\n",
      "Epoch [81/1200], Loss: 2967.1494\n",
      "Epoch [82/1200], Loss: 2967.1487\n",
      "Epoch [83/1200], Loss: 2967.1482\n",
      "Epoch [84/1200], Loss: 2967.1475\n",
      "Epoch [85/1200], Loss: 2967.1470\n",
      "Epoch [86/1200], Loss: 2967.1462\n",
      "Epoch [87/1200], Loss: 2967.1453\n",
      "Epoch [88/1200], Loss: 2967.1445\n",
      "Epoch [89/1200], Loss: 2967.1433\n",
      "Epoch [90/1200], Loss: 2967.1421\n",
      "Epoch [91/1200], Loss: 2967.1409\n",
      "Epoch [92/1200], Loss: 2967.1396\n",
      "Epoch [93/1200], Loss: 2967.1379\n",
      "Epoch [94/1200], Loss: 2967.1362\n",
      "Epoch [95/1200], Loss: 2967.1343\n",
      "Epoch [96/1200], Loss: 2967.1318\n",
      "Epoch [97/1200], Loss: 2967.1294\n",
      "Epoch [98/1200], Loss: 2967.1265\n",
      "Epoch [99/1200], Loss: 2967.1230\n",
      "Epoch [100/1200], Loss: 2967.1196\n",
      "Epoch [101/1200], Loss: 2967.1152\n",
      "Epoch [102/1200], Loss: 2967.1108\n",
      "Epoch [103/1200], Loss: 2967.1055\n",
      "Epoch [104/1200], Loss: 2967.0999\n",
      "Epoch [105/1200], Loss: 2967.0935\n",
      "Epoch [106/1200], Loss: 2967.0867\n",
      "Epoch [107/1200], Loss: 2967.0791\n",
      "Epoch [108/1200], Loss: 2967.0708\n",
      "Epoch [109/1200], Loss: 2967.0615\n",
      "Epoch [110/1200], Loss: 2967.0513\n",
      "Epoch [111/1200], Loss: 2967.0398\n",
      "Epoch [112/1200], Loss: 2967.0273\n",
      "Epoch [113/1200], Loss: 2967.0137\n",
      "Epoch [114/1200], Loss: 2966.9988\n",
      "Epoch [115/1200], Loss: 2966.9822\n",
      "Epoch [116/1200], Loss: 2966.9641\n",
      "Epoch [117/1200], Loss: 2966.9443\n",
      "Epoch [118/1200], Loss: 2966.9224\n",
      "Epoch [119/1200], Loss: 2966.8979\n",
      "Epoch [120/1200], Loss: 2966.8716\n",
      "Epoch [121/1200], Loss: 2966.8423\n",
      "Epoch [122/1200], Loss: 2966.8105\n",
      "Epoch [123/1200], Loss: 2966.7754\n",
      "Epoch [124/1200], Loss: 2966.7373\n",
      "Epoch [125/1200], Loss: 2966.6956\n",
      "Epoch [126/1200], Loss: 2966.6504\n",
      "Epoch [127/1200], Loss: 2966.6006\n",
      "Epoch [128/1200], Loss: 2966.5459\n",
      "Epoch [129/1200], Loss: 2966.4861\n",
      "Epoch [130/1200], Loss: 2966.4204\n",
      "Epoch [131/1200], Loss: 2966.3496\n",
      "Epoch [132/1200], Loss: 2966.2720\n",
      "Epoch [133/1200], Loss: 2966.1890\n",
      "Epoch [134/1200], Loss: 2966.1001\n",
      "Epoch [135/1200], Loss: 2966.0015\n",
      "Epoch [136/1200], Loss: 2965.8926\n",
      "Epoch [137/1200], Loss: 2965.7732\n",
      "Epoch [138/1200], Loss: 2965.6409\n",
      "Epoch [139/1200], Loss: 2965.4961\n",
      "Epoch [140/1200], Loss: 2965.3391\n",
      "Epoch [141/1200], Loss: 2965.1711\n",
      "Epoch [142/1200], Loss: 2964.9956\n",
      "Epoch [143/1200], Loss: 2964.8062\n",
      "Epoch [144/1200], Loss: 2964.6003\n",
      "Epoch [145/1200], Loss: 2964.3740\n",
      "Epoch [146/1200], Loss: 2964.1270\n",
      "Epoch [147/1200], Loss: 2963.8564\n",
      "Epoch [148/1200], Loss: 2963.5610\n",
      "Epoch [149/1200], Loss: 2963.2344\n",
      "Epoch [150/1200], Loss: 2962.8784\n",
      "Epoch [151/1200], Loss: 2962.4944\n",
      "Epoch [152/1200], Loss: 2962.0854\n",
      "Epoch [153/1200], Loss: 2961.6533\n",
      "Epoch [154/1200], Loss: 2961.1968\n",
      "Epoch [155/1200], Loss: 2960.6992\n",
      "Epoch [156/1200], Loss: 2960.1802\n",
      "Epoch [157/1200], Loss: 2959.6233\n",
      "Epoch [158/1200], Loss: 2959.0381\n",
      "Epoch [159/1200], Loss: 2958.3984\n",
      "Epoch [160/1200], Loss: 2957.7095\n",
      "Epoch [161/1200], Loss: 2956.9731\n",
      "Epoch [162/1200], Loss: 2956.1885\n",
      "Epoch [163/1200], Loss: 2955.3450\n",
      "Epoch [164/1200], Loss: 2954.4426\n",
      "Epoch [165/1200], Loss: 2953.4753\n",
      "Epoch [166/1200], Loss: 2952.4434\n",
      "Epoch [167/1200], Loss: 2951.3438\n",
      "Epoch [168/1200], Loss: 2950.1323\n",
      "Epoch [169/1200], Loss: 2948.8276\n",
      "Epoch [170/1200], Loss: 2947.4924\n",
      "Epoch [171/1200], Loss: 2946.0293\n",
      "Epoch [172/1200], Loss: 2944.5071\n",
      "Epoch [173/1200], Loss: 2942.9202\n",
      "Epoch [174/1200], Loss: 2941.2549\n",
      "Epoch [175/1200], Loss: 2939.5254\n",
      "Epoch [176/1200], Loss: 2937.7007\n",
      "Epoch [177/1200], Loss: 2935.7964\n",
      "Epoch [178/1200], Loss: 2933.7769\n",
      "Epoch [179/1200], Loss: 2931.6555\n",
      "Epoch [180/1200], Loss: 2929.3799\n",
      "Epoch [181/1200], Loss: 2927.0459\n",
      "Epoch [182/1200], Loss: 2924.5618\n",
      "Epoch [183/1200], Loss: 2922.0554\n",
      "Epoch [184/1200], Loss: 2919.4609\n",
      "Epoch [185/1200], Loss: 2916.5293\n",
      "Epoch [186/1200], Loss: 2913.7532\n",
      "Epoch [187/1200], Loss: 2910.6118\n",
      "Epoch [188/1200], Loss: 2907.2759\n",
      "Epoch [189/1200], Loss: 2903.3687\n",
      "Epoch [190/1200], Loss: 2899.9771\n",
      "Epoch [191/1200], Loss: 2895.3750\n",
      "Epoch [192/1200], Loss: 2891.1023\n",
      "Epoch [193/1200], Loss: 2886.0510\n",
      "Epoch [194/1200], Loss: 2880.6213\n",
      "Epoch [195/1200], Loss: 2874.9707\n",
      "Epoch [196/1200], Loss: 2868.4187\n",
      "Epoch [197/1200], Loss: 2861.7588\n",
      "Epoch [198/1200], Loss: 2853.7908\n",
      "Epoch [199/1200], Loss: 2846.0176\n",
      "Epoch [200/1200], Loss: 2836.5847\n",
      "Epoch [201/1200], Loss: 2827.2334\n",
      "Epoch [202/1200], Loss: 2816.2944\n",
      "Epoch [203/1200], Loss: 2805.2168\n",
      "Epoch [204/1200], Loss: 2792.8665\n",
      "Epoch [205/1200], Loss: 2780.1401\n",
      "Epoch [206/1200], Loss: 2766.2983\n",
      "Epoch [207/1200], Loss: 2752.9688\n",
      "Epoch [208/1200], Loss: 2737.5876\n",
      "Epoch [209/1200], Loss: 2722.3623\n",
      "Epoch [210/1200], Loss: 2705.4177\n",
      "Epoch [211/1200], Loss: 2690.9373\n",
      "Epoch [212/1200], Loss: 2672.9133\n",
      "Epoch [213/1200], Loss: 2657.0054\n",
      "Epoch [214/1200], Loss: 2640.5645\n",
      "Epoch [215/1200], Loss: 2624.0083\n",
      "Epoch [216/1200], Loss: 2609.2339\n",
      "Epoch [217/1200], Loss: 2593.9529\n",
      "Epoch [218/1200], Loss: 2582.1436\n",
      "Epoch [219/1200], Loss: 2569.0186\n",
      "Epoch [220/1200], Loss: 2558.9131\n",
      "Epoch [221/1200], Loss: 2548.3398\n",
      "Epoch [222/1200], Loss: 2538.6484\n",
      "Epoch [223/1200], Loss: 2526.5889\n",
      "Epoch [224/1200], Loss: 2518.3542\n",
      "Epoch [225/1200], Loss: 2515.6587\n",
      "Epoch [226/1200], Loss: 2493.7925\n",
      "Epoch [227/1200], Loss: 2477.2952\n",
      "Epoch [228/1200], Loss: 2468.0059\n",
      "Epoch [229/1200], Loss: 2455.4678\n",
      "Epoch [230/1200], Loss: 2443.7458\n",
      "Epoch [231/1200], Loss: 2431.1431\n",
      "Epoch [232/1200], Loss: 2420.5161\n",
      "Epoch [233/1200], Loss: 2408.8301\n",
      "Epoch [234/1200], Loss: 2396.7212\n",
      "Epoch [235/1200], Loss: 2382.7676\n",
      "Epoch [236/1200], Loss: 2372.1719\n",
      "Epoch [237/1200], Loss: 2360.9978\n",
      "Epoch [238/1200], Loss: 2360.1792\n",
      "Epoch [239/1200], Loss: 2336.1980\n",
      "Epoch [240/1200], Loss: 2363.1343\n",
      "Epoch [241/1200], Loss: 2323.3057\n",
      "Epoch [242/1200], Loss: 2333.5378\n",
      "Epoch [243/1200], Loss: 2314.8833\n",
      "Epoch [244/1200], Loss: 2277.8455\n",
      "Epoch [245/1200], Loss: 2265.4312\n",
      "Epoch [246/1200], Loss: 2255.6089\n",
      "Epoch [247/1200], Loss: 2222.6562\n",
      "Epoch [248/1200], Loss: 2208.3120\n",
      "Epoch [249/1200], Loss: 2192.3018\n",
      "Epoch [250/1200], Loss: 2170.7527\n",
      "Epoch [251/1200], Loss: 2143.2107\n",
      "Epoch [252/1200], Loss: 2116.4634\n",
      "Epoch [253/1200], Loss: 2095.9727\n",
      "Epoch [254/1200], Loss: 2077.9202\n",
      "Epoch [255/1200], Loss: 2059.4492\n",
      "Epoch [256/1200], Loss: 2040.7329\n",
      "Epoch [257/1200], Loss: 2027.4448\n",
      "Epoch [258/1200], Loss: 2015.0944\n",
      "Epoch [259/1200], Loss: 2003.9634\n",
      "Epoch [260/1200], Loss: 1992.8226\n",
      "Epoch [261/1200], Loss: 1983.4264\n",
      "Epoch [262/1200], Loss: 1978.5029\n",
      "Epoch [263/1200], Loss: 1969.5327\n",
      "Epoch [264/1200], Loss: 1968.8068\n",
      "Epoch [265/1200], Loss: 1958.0260\n",
      "Epoch [266/1200], Loss: 1952.1991\n",
      "Epoch [267/1200], Loss: 1950.3389\n",
      "Epoch [268/1200], Loss: 1945.6401\n",
      "Epoch [269/1200], Loss: 1932.7161\n",
      "Epoch [270/1200], Loss: 1944.4496\n",
      "Epoch [271/1200], Loss: 1921.9858\n",
      "Epoch [272/1200], Loss: 1920.5728\n",
      "Epoch [273/1200], Loss: 1910.6064\n",
      "Epoch [274/1200], Loss: 1908.4375\n",
      "Epoch [275/1200], Loss: 1902.0920\n",
      "Epoch [276/1200], Loss: 1899.9203\n",
      "Epoch [277/1200], Loss: 1895.7039\n",
      "Epoch [278/1200], Loss: 1890.0779\n",
      "Epoch [279/1200], Loss: 1887.1743\n",
      "Epoch [280/1200], Loss: 1885.3298\n",
      "Epoch [281/1200], Loss: 1882.2329\n",
      "Epoch [282/1200], Loss: 1878.4338\n",
      "Epoch [283/1200], Loss: 1874.5055\n",
      "Epoch [284/1200], Loss: 1871.9945\n",
      "Epoch [285/1200], Loss: 1867.8005\n",
      "Epoch [286/1200], Loss: 1865.7051\n",
      "Epoch [287/1200], Loss: 1862.2268\n",
      "Epoch [288/1200], Loss: 1856.5957\n",
      "Epoch [289/1200], Loss: 1857.2690\n",
      "Epoch [290/1200], Loss: 1854.9585\n",
      "Epoch [291/1200], Loss: 1848.9904\n",
      "Epoch [292/1200], Loss: 1848.0605\n",
      "Epoch [293/1200], Loss: 1840.7078\n",
      "Epoch [294/1200], Loss: 1839.7976\n",
      "Epoch [295/1200], Loss: 1835.3026\n",
      "Epoch [296/1200], Loss: 1833.7727\n",
      "Epoch [297/1200], Loss: 1830.3274\n",
      "Epoch [298/1200], Loss: 1826.9438\n",
      "Epoch [299/1200], Loss: 1823.5477\n",
      "Epoch [300/1200], Loss: 1820.8010\n",
      "Epoch [301/1200], Loss: 1817.8945\n",
      "Epoch [302/1200], Loss: 1813.7307\n",
      "Epoch [303/1200], Loss: 1809.8066\n",
      "Epoch [304/1200], Loss: 1804.2505\n",
      "Epoch [305/1200], Loss: 1801.2689\n",
      "Epoch [306/1200], Loss: 1796.0570\n",
      "Epoch [307/1200], Loss: 1790.2905\n",
      "Epoch [308/1200], Loss: 1785.5118\n",
      "Epoch [309/1200], Loss: 1778.0339\n",
      "Epoch [310/1200], Loss: 1772.8998\n",
      "Epoch [311/1200], Loss: 1765.8413\n",
      "Epoch [312/1200], Loss: 1759.5752\n",
      "Epoch [313/1200], Loss: 1751.5895\n",
      "Epoch [314/1200], Loss: 1745.3163\n",
      "Epoch [315/1200], Loss: 1736.7805\n",
      "Epoch [316/1200], Loss: 1729.0962\n",
      "Epoch [317/1200], Loss: 1720.2930\n",
      "Epoch [318/1200], Loss: 1711.6067\n",
      "Epoch [319/1200], Loss: 1700.4670\n",
      "Epoch [320/1200], Loss: 1690.0842\n",
      "Epoch [321/1200], Loss: 1679.4094\n",
      "Epoch [322/1200], Loss: 1665.0532\n",
      "Epoch [323/1200], Loss: 1654.3899\n",
      "Epoch [324/1200], Loss: 1640.8857\n",
      "Epoch [325/1200], Loss: 1624.7751\n",
      "Epoch [326/1200], Loss: 1615.5308\n",
      "Epoch [327/1200], Loss: 1596.1790\n",
      "Epoch [328/1200], Loss: 1583.0861\n",
      "Epoch [329/1200], Loss: 1567.8313\n",
      "Epoch [330/1200], Loss: 1552.0016\n",
      "Epoch [331/1200], Loss: 1538.5629\n",
      "Epoch [332/1200], Loss: 1524.4840\n",
      "Epoch [333/1200], Loss: 1510.0099\n",
      "Epoch [334/1200], Loss: 1497.5767\n",
      "Epoch [335/1200], Loss: 1486.0183\n",
      "Epoch [336/1200], Loss: 1475.3860\n",
      "Epoch [337/1200], Loss: 1464.4404\n",
      "Epoch [338/1200], Loss: 1455.7865\n",
      "Epoch [339/1200], Loss: 1447.2634\n",
      "Epoch [340/1200], Loss: 1442.1494\n",
      "Epoch [341/1200], Loss: 1428.1755\n",
      "Epoch [342/1200], Loss: 1419.3442\n",
      "Epoch [343/1200], Loss: 1413.3911\n",
      "Epoch [344/1200], Loss: 1404.1201\n",
      "Epoch [345/1200], Loss: 1393.1204\n",
      "Epoch [346/1200], Loss: 1387.6902\n",
      "Epoch [347/1200], Loss: 1380.3538\n",
      "Epoch [348/1200], Loss: 1371.5500\n",
      "Epoch [349/1200], Loss: 1362.4407\n",
      "Epoch [350/1200], Loss: 1353.5925\n",
      "Epoch [351/1200], Loss: 1346.2192\n",
      "Epoch [352/1200], Loss: 1336.4104\n",
      "Epoch [353/1200], Loss: 1329.0254\n",
      "Epoch [354/1200], Loss: 1320.5105\n",
      "Epoch [355/1200], Loss: 1312.4983\n",
      "Epoch [356/1200], Loss: 1304.8391\n",
      "Epoch [357/1200], Loss: 1293.9192\n",
      "Epoch [358/1200], Loss: 1288.0745\n",
      "Epoch [359/1200], Loss: 1279.4158\n",
      "Epoch [360/1200], Loss: 1274.4885\n",
      "Epoch [361/1200], Loss: 1265.1904\n",
      "Epoch [362/1200], Loss: 1257.8462\n",
      "Epoch [363/1200], Loss: 1246.8154\n",
      "Epoch [364/1200], Loss: 1243.3740\n",
      "Epoch [365/1200], Loss: 1244.4714\n",
      "Epoch [366/1200], Loss: 1227.1177\n",
      "Epoch [367/1200], Loss: 1222.7916\n",
      "Epoch [368/1200], Loss: 1220.0623\n",
      "Epoch [369/1200], Loss: 1204.6934\n",
      "Epoch [370/1200], Loss: 1199.0146\n",
      "Epoch [371/1200], Loss: 1197.6477\n",
      "Epoch [372/1200], Loss: 1180.0378\n",
      "Epoch [373/1200], Loss: 1184.9214\n",
      "Epoch [374/1200], Loss: 1173.6469\n",
      "Epoch [375/1200], Loss: 1163.9513\n",
      "Epoch [376/1200], Loss: 1159.3341\n",
      "Epoch [377/1200], Loss: 1145.3341\n",
      "Epoch [378/1200], Loss: 1145.6597\n",
      "Epoch [379/1200], Loss: 1130.2495\n",
      "Epoch [380/1200], Loss: 1126.2771\n",
      "Epoch [381/1200], Loss: 1114.1356\n",
      "Epoch [382/1200], Loss: 1107.8853\n",
      "Epoch [383/1200], Loss: 1097.4480\n",
      "Epoch [384/1200], Loss: 1089.7542\n",
      "Epoch [385/1200], Loss: 1084.6116\n",
      "Epoch [386/1200], Loss: 1070.1047\n",
      "Epoch [387/1200], Loss: 1068.5370\n",
      "Epoch [388/1200], Loss: 1063.6260\n",
      "Epoch [389/1200], Loss: 1050.3076\n",
      "Epoch [390/1200], Loss: 1054.4694\n",
      "Epoch [391/1200], Loss: 1048.8392\n",
      "Epoch [392/1200], Loss: 1045.6355\n",
      "Epoch [393/1200], Loss: 1029.0984\n",
      "Epoch [394/1200], Loss: 1023.7019\n",
      "Epoch [395/1200], Loss: 1014.7305\n",
      "Epoch [396/1200], Loss: 1008.5793\n",
      "Epoch [397/1200], Loss: 1001.2792\n",
      "Epoch [398/1200], Loss: 988.8640\n",
      "Epoch [399/1200], Loss: 993.1987\n",
      "Epoch [400/1200], Loss: 987.4662\n",
      "Epoch [401/1200], Loss: 971.6251\n",
      "Epoch [402/1200], Loss: 968.5661\n",
      "Epoch [403/1200], Loss: 958.3319\n",
      "Epoch [404/1200], Loss: 951.5913\n",
      "Epoch [405/1200], Loss: 943.7954\n",
      "Epoch [406/1200], Loss: 939.4117\n",
      "Epoch [407/1200], Loss: 929.6831\n",
      "Epoch [408/1200], Loss: 932.0422\n",
      "Epoch [409/1200], Loss: 936.6583\n",
      "Epoch [410/1200], Loss: 926.6011\n",
      "Epoch [411/1200], Loss: 915.7717\n",
      "Epoch [412/1200], Loss: 919.0310\n",
      "Epoch [413/1200], Loss: 894.3121\n",
      "Epoch [414/1200], Loss: 903.9160\n",
      "Epoch [415/1200], Loss: 880.6389\n",
      "Epoch [416/1200], Loss: 881.6044\n",
      "Epoch [417/1200], Loss: 871.0653\n",
      "Epoch [418/1200], Loss: 871.6773\n",
      "Epoch [419/1200], Loss: 864.4949\n",
      "Epoch [420/1200], Loss: 862.4054\n",
      "Epoch [421/1200], Loss: 847.0139\n",
      "Epoch [422/1200], Loss: 841.5966\n",
      "Epoch [423/1200], Loss: 836.8073\n",
      "Epoch [424/1200], Loss: 837.4592\n",
      "Epoch [425/1200], Loss: 834.6096\n",
      "Epoch [426/1200], Loss: 841.6179\n",
      "Epoch [427/1200], Loss: 817.8091\n",
      "Epoch [428/1200], Loss: 823.7936\n",
      "Epoch [429/1200], Loss: 818.4808\n",
      "Epoch [430/1200], Loss: 807.0431\n",
      "Epoch [431/1200], Loss: 802.0312\n",
      "Epoch [432/1200], Loss: 799.9274\n",
      "Epoch [433/1200], Loss: 810.8776\n",
      "Epoch [434/1200], Loss: 795.1012\n",
      "Epoch [435/1200], Loss: 801.2672\n",
      "Epoch [436/1200], Loss: 788.8422\n",
      "Epoch [437/1200], Loss: 783.6951\n",
      "Epoch [438/1200], Loss: 784.5237\n",
      "Epoch [439/1200], Loss: 770.6797\n",
      "Epoch [440/1200], Loss: 762.6501\n",
      "Epoch [441/1200], Loss: 762.2562\n",
      "Epoch [442/1200], Loss: 760.2226\n",
      "Epoch [443/1200], Loss: 750.7455\n",
      "Epoch [444/1200], Loss: 749.2057\n",
      "Epoch [445/1200], Loss: 745.0444\n",
      "Epoch [446/1200], Loss: 738.1635\n",
      "Epoch [447/1200], Loss: 736.5425\n",
      "Epoch [448/1200], Loss: 733.6727\n",
      "Epoch [449/1200], Loss: 726.0753\n",
      "Epoch [450/1200], Loss: 724.2982\n",
      "Epoch [451/1200], Loss: 724.1406\n",
      "Epoch [452/1200], Loss: 715.0453\n",
      "Epoch [453/1200], Loss: 710.6567\n",
      "Epoch [454/1200], Loss: 707.1980\n",
      "Epoch [455/1200], Loss: 703.9403\n",
      "Epoch [456/1200], Loss: 702.1776\n",
      "Epoch [457/1200], Loss: 701.3522\n",
      "Epoch [458/1200], Loss: 696.8798\n",
      "Epoch [459/1200], Loss: 700.7535\n",
      "Epoch [460/1200], Loss: 708.1511\n",
      "Epoch [461/1200], Loss: 703.1030\n",
      "Epoch [462/1200], Loss: 708.8234\n",
      "Epoch [463/1200], Loss: 701.2359\n",
      "Epoch [464/1200], Loss: 682.9656\n",
      "Epoch [465/1200], Loss: 686.6735\n",
      "Epoch [466/1200], Loss: 708.4293\n",
      "Epoch [467/1200], Loss: 693.2737\n",
      "Epoch [468/1200], Loss: 682.4738\n",
      "Epoch [469/1200], Loss: 679.2148\n",
      "Epoch [470/1200], Loss: 683.3288\n",
      "Epoch [471/1200], Loss: 668.8401\n",
      "Epoch [472/1200], Loss: 668.7716\n",
      "Epoch [473/1200], Loss: 670.8476\n",
      "Epoch [474/1200], Loss: 661.1869\n",
      "Epoch [475/1200], Loss: 659.1233\n",
      "Epoch [476/1200], Loss: 660.1084\n",
      "Epoch [477/1200], Loss: 651.2951\n",
      "Epoch [478/1200], Loss: 651.9376\n",
      "Epoch [479/1200], Loss: 649.9169\n",
      "Epoch [480/1200], Loss: 647.0305\n",
      "Epoch [481/1200], Loss: 646.3740\n",
      "Epoch [482/1200], Loss: 643.4642\n",
      "Epoch [483/1200], Loss: 637.1634\n",
      "Epoch [484/1200], Loss: 634.7131\n",
      "Epoch [485/1200], Loss: 637.0519\n",
      "Epoch [486/1200], Loss: 636.4022\n",
      "Epoch [487/1200], Loss: 637.6909\n",
      "Epoch [488/1200], Loss: 631.7145\n",
      "Epoch [489/1200], Loss: 626.7262\n",
      "Epoch [490/1200], Loss: 630.8120\n",
      "Epoch [491/1200], Loss: 636.7625\n",
      "Epoch [492/1200], Loss: 630.1031\n",
      "Epoch [493/1200], Loss: 626.9469\n",
      "Epoch [494/1200], Loss: 627.6768\n",
      "Epoch [495/1200], Loss: 625.9060\n",
      "Epoch [496/1200], Loss: 615.5715\n",
      "Epoch [497/1200], Loss: 614.3059\n",
      "Epoch [498/1200], Loss: 612.9548\n",
      "Epoch [499/1200], Loss: 609.7637\n",
      "Epoch [500/1200], Loss: 610.0112\n",
      "Epoch [501/1200], Loss: 605.8858\n",
      "Epoch [502/1200], Loss: 604.3133\n",
      "Epoch [503/1200], Loss: 600.0848\n",
      "Epoch [504/1200], Loss: 600.2501\n",
      "Epoch [505/1200], Loss: 598.1558\n",
      "Epoch [506/1200], Loss: 596.6522\n",
      "Epoch [507/1200], Loss: 594.0626\n",
      "Epoch [508/1200], Loss: 592.7245\n",
      "Epoch [509/1200], Loss: 589.5963\n",
      "Epoch [510/1200], Loss: 589.9442\n",
      "Epoch [511/1200], Loss: 587.2136\n",
      "Epoch [512/1200], Loss: 586.7324\n",
      "Epoch [513/1200], Loss: 583.8613\n",
      "Epoch [514/1200], Loss: 582.3688\n",
      "Epoch [515/1200], Loss: 578.6833\n",
      "Epoch [516/1200], Loss: 579.0736\n",
      "Epoch [517/1200], Loss: 577.7048\n",
      "Epoch [518/1200], Loss: 578.3959\n",
      "Epoch [519/1200], Loss: 578.2626\n",
      "Epoch [520/1200], Loss: 577.0704\n",
      "Epoch [521/1200], Loss: 571.3873\n",
      "Epoch [522/1200], Loss: 567.9836\n",
      "Epoch [523/1200], Loss: 567.6295\n",
      "Epoch [524/1200], Loss: 569.5504\n",
      "Epoch [525/1200], Loss: 569.3204\n",
      "Epoch [526/1200], Loss: 569.5148\n",
      "Epoch [527/1200], Loss: 564.7644\n",
      "Epoch [528/1200], Loss: 559.0668\n",
      "Epoch [529/1200], Loss: 558.4437\n",
      "Epoch [530/1200], Loss: 559.6446\n",
      "Epoch [531/1200], Loss: 560.2834\n",
      "Epoch [532/1200], Loss: 555.7363\n",
      "Epoch [533/1200], Loss: 551.7552\n",
      "Epoch [534/1200], Loss: 550.1554\n",
      "Epoch [535/1200], Loss: 549.9508\n",
      "Epoch [536/1200], Loss: 549.4034\n",
      "Epoch [537/1200], Loss: 546.0739\n",
      "Epoch [538/1200], Loss: 544.0194\n",
      "Epoch [539/1200], Loss: 541.9735\n",
      "Epoch [540/1200], Loss: 541.6623\n",
      "Epoch [541/1200], Loss: 539.8751\n",
      "Epoch [542/1200], Loss: 537.8450\n",
      "Epoch [543/1200], Loss: 536.1168\n",
      "Epoch [544/1200], Loss: 535.2311\n",
      "Epoch [545/1200], Loss: 533.8455\n",
      "Epoch [546/1200], Loss: 532.9542\n",
      "Epoch [547/1200], Loss: 531.6102\n",
      "Epoch [548/1200], Loss: 530.2457\n",
      "Epoch [549/1200], Loss: 528.3937\n",
      "Epoch [550/1200], Loss: 527.1309\n",
      "Epoch [551/1200], Loss: 525.1484\n",
      "Epoch [552/1200], Loss: 523.7756\n",
      "Epoch [553/1200], Loss: 521.9808\n",
      "Epoch [554/1200], Loss: 520.3348\n",
      "Epoch [555/1200], Loss: 519.0995\n",
      "Epoch [556/1200], Loss: 517.7440\n",
      "Epoch [557/1200], Loss: 516.3533\n",
      "Epoch [558/1200], Loss: 514.7917\n",
      "Epoch [559/1200], Loss: 513.5056\n",
      "Epoch [560/1200], Loss: 512.0311\n",
      "Epoch [561/1200], Loss: 510.4891\n",
      "Epoch [562/1200], Loss: 508.9650\n",
      "Epoch [563/1200], Loss: 507.4168\n",
      "Epoch [564/1200], Loss: 506.3502\n",
      "Epoch [565/1200], Loss: 505.0432\n",
      "Epoch [566/1200], Loss: 503.6067\n",
      "Epoch [567/1200], Loss: 501.9680\n",
      "Epoch [568/1200], Loss: 500.3966\n",
      "Epoch [569/1200], Loss: 498.7727\n",
      "Epoch [570/1200], Loss: 497.6691\n",
      "Epoch [571/1200], Loss: 497.3412\n",
      "Epoch [572/1200], Loss: 498.8707\n",
      "Epoch [573/1200], Loss: 507.5598\n",
      "Epoch [574/1200], Loss: 544.7551\n",
      "Epoch [575/1200], Loss: 619.9213\n",
      "Epoch [576/1200], Loss: 933.5910\n",
      "Epoch [577/1200], Loss: 585.7194\n",
      "Epoch [578/1200], Loss: 970.9983\n",
      "Epoch [579/1200], Loss: 832.5566\n",
      "Epoch [580/1200], Loss: 1122.4592\n",
      "Epoch [581/1200], Loss: 889.6943\n",
      "Epoch [582/1200], Loss: 685.6892\n",
      "Epoch [583/1200], Loss: 904.7769\n",
      "Epoch [584/1200], Loss: 661.2980\n",
      "Epoch [585/1200], Loss: 755.5517\n",
      "Epoch [586/1200], Loss: 737.9557\n",
      "Epoch [587/1200], Loss: 709.8004\n",
      "Epoch [588/1200], Loss: 732.2067\n",
      "Epoch [589/1200], Loss: 617.2839\n",
      "Epoch [590/1200], Loss: 592.7999\n",
      "Epoch [591/1200], Loss: 675.8870\n",
      "Epoch [592/1200], Loss: 668.4960\n",
      "Epoch [593/1200], Loss: 611.1354\n",
      "Epoch [594/1200], Loss: 583.7733\n",
      "Epoch [595/1200], Loss: 604.2969\n",
      "Epoch [596/1200], Loss: 623.0122\n",
      "Epoch [597/1200], Loss: 583.7207\n",
      "Epoch [598/1200], Loss: 560.8787\n",
      "Epoch [599/1200], Loss: 588.7848\n",
      "Epoch [600/1200], Loss: 586.8386\n",
      "Epoch [601/1200], Loss: 576.8901\n",
      "Epoch [602/1200], Loss: 545.6398\n",
      "Epoch [603/1200], Loss: 566.9546\n",
      "Epoch [604/1200], Loss: 569.0905\n",
      "Epoch [605/1200], Loss: 547.2759\n",
      "Epoch [606/1200], Loss: 543.0001\n",
      "Epoch [607/1200], Loss: 528.0100\n",
      "Epoch [608/1200], Loss: 527.7836\n",
      "Epoch [609/1200], Loss: 532.4479\n",
      "Epoch [610/1200], Loss: 512.7213\n",
      "Epoch [611/1200], Loss: 509.1635\n",
      "Epoch [612/1200], Loss: 512.6205\n",
      "Epoch [613/1200], Loss: 510.7119\n",
      "Epoch [614/1200], Loss: 504.3535\n",
      "Epoch [615/1200], Loss: 504.9659\n",
      "Epoch [616/1200], Loss: 500.8436\n",
      "Epoch [617/1200], Loss: 494.0842\n",
      "Epoch [618/1200], Loss: 489.9304\n",
      "Epoch [619/1200], Loss: 488.7978\n",
      "Epoch [620/1200], Loss: 489.3159\n",
      "Epoch [621/1200], Loss: 486.3122\n",
      "Epoch [622/1200], Loss: 481.3284\n",
      "Epoch [623/1200], Loss: 481.3922\n",
      "Epoch [624/1200], Loss: 480.4053\n",
      "Epoch [625/1200], Loss: 476.1458\n",
      "Epoch [626/1200], Loss: 474.0287\n",
      "Epoch [627/1200], Loss: 474.0951\n",
      "Epoch [628/1200], Loss: 471.9008\n",
      "Epoch [629/1200], Loss: 469.0070\n",
      "Epoch [630/1200], Loss: 467.3735\n",
      "Epoch [631/1200], Loss: 466.5820\n",
      "Epoch [632/1200], Loss: 464.1996\n",
      "Epoch [633/1200], Loss: 462.3982\n",
      "Epoch [634/1200], Loss: 462.4557\n",
      "Epoch [635/1200], Loss: 460.4112\n",
      "Epoch [636/1200], Loss: 458.0337\n",
      "Epoch [637/1200], Loss: 457.3755\n",
      "Epoch [638/1200], Loss: 455.9388\n",
      "Epoch [639/1200], Loss: 454.3019\n",
      "Epoch [640/1200], Loss: 453.3730\n",
      "Epoch [641/1200], Loss: 452.2236\n",
      "Epoch [642/1200], Loss: 450.7692\n",
      "Epoch [643/1200], Loss: 449.6120\n",
      "Epoch [644/1200], Loss: 448.7238\n",
      "Epoch [645/1200], Loss: 447.2022\n",
      "Epoch [646/1200], Loss: 445.9207\n",
      "Epoch [647/1200], Loss: 445.2511\n",
      "Epoch [648/1200], Loss: 443.8157\n",
      "Epoch [649/1200], Loss: 442.7003\n",
      "Epoch [650/1200], Loss: 441.7845\n",
      "Epoch [651/1200], Loss: 440.4883\n",
      "Epoch [652/1200], Loss: 439.3052\n",
      "Epoch [653/1200], Loss: 438.4079\n",
      "Epoch [654/1200], Loss: 437.2851\n",
      "Epoch [655/1200], Loss: 436.1587\n",
      "Epoch [656/1200], Loss: 435.2638\n",
      "Epoch [657/1200], Loss: 434.0245\n",
      "Epoch [658/1200], Loss: 432.9593\n",
      "Epoch [659/1200], Loss: 432.0609\n",
      "Epoch [660/1200], Loss: 430.9031\n",
      "Epoch [661/1200], Loss: 429.7935\n",
      "Epoch [662/1200], Loss: 428.7371\n",
      "Epoch [663/1200], Loss: 427.5153\n",
      "Epoch [664/1200], Loss: 426.4684\n",
      "Epoch [665/1200], Loss: 425.4510\n",
      "Epoch [666/1200], Loss: 424.3965\n",
      "Epoch [667/1200], Loss: 423.4152\n",
      "Epoch [668/1200], Loss: 422.3560\n",
      "Epoch [669/1200], Loss: 421.3490\n",
      "Epoch [670/1200], Loss: 420.3624\n",
      "Epoch [671/1200], Loss: 419.4140\n",
      "Epoch [672/1200], Loss: 418.3774\n",
      "Epoch [673/1200], Loss: 417.3815\n",
      "Epoch [674/1200], Loss: 416.4336\n",
      "Epoch [675/1200], Loss: 415.4902\n",
      "Epoch [676/1200], Loss: 414.5429\n",
      "Epoch [677/1200], Loss: 413.3608\n",
      "Epoch [678/1200], Loss: 412.4432\n",
      "Epoch [679/1200], Loss: 411.3966\n",
      "Epoch [680/1200], Loss: 410.2939\n",
      "Epoch [681/1200], Loss: 409.3510\n",
      "Epoch [682/1200], Loss: 408.4293\n",
      "Epoch [683/1200], Loss: 407.4201\n",
      "Epoch [684/1200], Loss: 406.4109\n",
      "Epoch [685/1200], Loss: 405.4110\n",
      "Epoch [686/1200], Loss: 404.4120\n",
      "Epoch [687/1200], Loss: 403.2571\n",
      "Epoch [688/1200], Loss: 402.3475\n",
      "Epoch [689/1200], Loss: 401.1969\n",
      "Epoch [690/1200], Loss: 400.2882\n",
      "Epoch [691/1200], Loss: 399.2794\n",
      "Epoch [692/1200], Loss: 398.1332\n",
      "Epoch [693/1200], Loss: 397.1672\n",
      "Epoch [694/1200], Loss: 396.0993\n",
      "Epoch [695/1200], Loss: 395.0084\n",
      "Epoch [696/1200], Loss: 394.0049\n",
      "Epoch [697/1200], Loss: 393.0665\n",
      "Epoch [698/1200], Loss: 392.0107\n",
      "Epoch [699/1200], Loss: 391.0247\n",
      "Epoch [700/1200], Loss: 389.9872\n",
      "Epoch [701/1200], Loss: 388.9373\n",
      "Epoch [702/1200], Loss: 387.8613\n",
      "Epoch [703/1200], Loss: 386.8614\n",
      "Epoch [704/1200], Loss: 385.7826\n",
      "Epoch [705/1200], Loss: 384.8432\n",
      "Epoch [706/1200], Loss: 383.7468\n",
      "Epoch [707/1200], Loss: 382.6796\n",
      "Epoch [708/1200], Loss: 381.5984\n",
      "Epoch [709/1200], Loss: 380.6046\n",
      "Epoch [710/1200], Loss: 379.5293\n",
      "Epoch [711/1200], Loss: 378.4056\n",
      "Epoch [712/1200], Loss: 377.4630\n",
      "Epoch [713/1200], Loss: 376.3763\n",
      "Epoch [714/1200], Loss: 375.2957\n",
      "Epoch [715/1200], Loss: 374.2228\n",
      "Epoch [716/1200], Loss: 373.1451\n",
      "Epoch [717/1200], Loss: 372.1852\n",
      "Epoch [718/1200], Loss: 371.0113\n",
      "Epoch [719/1200], Loss: 369.9807\n",
      "Epoch [720/1200], Loss: 369.0349\n",
      "Epoch [721/1200], Loss: 367.8904\n",
      "Epoch [722/1200], Loss: 366.9875\n",
      "Epoch [723/1200], Loss: 365.9387\n",
      "Epoch [724/1200], Loss: 364.7239\n",
      "Epoch [725/1200], Loss: 363.9197\n",
      "Epoch [726/1200], Loss: 362.7994\n",
      "Epoch [727/1200], Loss: 361.7773\n",
      "Epoch [728/1200], Loss: 360.7818\n",
      "Epoch [729/1200], Loss: 359.8459\n",
      "Epoch [730/1200], Loss: 358.8889\n",
      "Epoch [731/1200], Loss: 357.8247\n",
      "Epoch [732/1200], Loss: 356.8275\n",
      "Epoch [733/1200], Loss: 355.5927\n",
      "Epoch [734/1200], Loss: 354.8056\n",
      "Epoch [735/1200], Loss: 353.8162\n",
      "Epoch [736/1200], Loss: 352.7806\n",
      "Epoch [737/1200], Loss: 351.8847\n",
      "Epoch [738/1200], Loss: 350.6274\n",
      "Epoch [739/1200], Loss: 349.8282\n",
      "Epoch [740/1200], Loss: 348.6909\n",
      "Epoch [741/1200], Loss: 347.7444\n",
      "Epoch [742/1200], Loss: 346.7093\n",
      "Epoch [743/1200], Loss: 345.8744\n",
      "Epoch [744/1200], Loss: 344.8294\n",
      "Epoch [745/1200], Loss: 343.8360\n",
      "Epoch [746/1200], Loss: 342.8546\n",
      "Epoch [747/1200], Loss: 341.9914\n",
      "Epoch [748/1200], Loss: 340.9587\n",
      "Epoch [749/1200], Loss: 339.8367\n",
      "Epoch [750/1200], Loss: 338.9196\n",
      "Epoch [751/1200], Loss: 338.1166\n",
      "Epoch [752/1200], Loss: 336.9738\n",
      "Epoch [753/1200], Loss: 336.0901\n",
      "Epoch [754/1200], Loss: 334.9799\n",
      "Epoch [755/1200], Loss: 333.9366\n",
      "Epoch [756/1200], Loss: 333.0219\n",
      "Epoch [757/1200], Loss: 332.0539\n",
      "Epoch [758/1200], Loss: 331.2591\n",
      "Epoch [759/1200], Loss: 330.4610\n",
      "Epoch [760/1200], Loss: 329.2349\n",
      "Epoch [761/1200], Loss: 328.0168\n",
      "Epoch [762/1200], Loss: 327.1285\n",
      "Epoch [763/1200], Loss: 326.1919\n",
      "Epoch [764/1200], Loss: 325.5342\n",
      "Epoch [765/1200], Loss: 324.4936\n",
      "Epoch [766/1200], Loss: 323.1271\n",
      "Epoch [767/1200], Loss: 321.8959\n",
      "Epoch [768/1200], Loss: 320.9630\n",
      "Epoch [769/1200], Loss: 320.1379\n",
      "Epoch [770/1200], Loss: 319.4868\n",
      "Epoch [771/1200], Loss: 318.1946\n",
      "Epoch [772/1200], Loss: 316.7889\n",
      "Epoch [773/1200], Loss: 315.6451\n",
      "Epoch [774/1200], Loss: 314.5748\n",
      "Epoch [775/1200], Loss: 313.5677\n",
      "Epoch [776/1200], Loss: 312.5388\n",
      "Epoch [777/1200], Loss: 311.5719\n",
      "Epoch [778/1200], Loss: 310.8960\n",
      "Epoch [779/1200], Loss: 310.9170\n",
      "Epoch [780/1200], Loss: 310.7209\n",
      "Epoch [781/1200], Loss: 311.3901\n",
      "Epoch [782/1200], Loss: 309.7456\n",
      "Epoch [783/1200], Loss: 308.3203\n",
      "Epoch [784/1200], Loss: 305.3200\n",
      "Epoch [785/1200], Loss: 303.5100\n",
      "Epoch [786/1200], Loss: 302.5319\n",
      "Epoch [787/1200], Loss: 302.8497\n",
      "Epoch [788/1200], Loss: 304.1198\n",
      "Epoch [789/1200], Loss: 303.5584\n",
      "Epoch [790/1200], Loss: 302.9025\n",
      "Epoch [791/1200], Loss: 300.0266\n",
      "Epoch [792/1200], Loss: 297.8280\n",
      "Epoch [793/1200], Loss: 296.0887\n",
      "Epoch [794/1200], Loss: 294.8309\n",
      "Epoch [795/1200], Loss: 294.6839\n",
      "Epoch [796/1200], Loss: 294.9189\n",
      "Epoch [797/1200], Loss: 296.2295\n",
      "Epoch [798/1200], Loss: 295.3073\n",
      "Epoch [799/1200], Loss: 295.3282\n",
      "Epoch [800/1200], Loss: 292.1098\n",
      "Epoch [801/1200], Loss: 289.9791\n",
      "Epoch [802/1200], Loss: 287.7333\n",
      "Epoch [803/1200], Loss: 286.0823\n",
      "Epoch [804/1200], Loss: 285.2717\n",
      "Epoch [805/1200], Loss: 284.9034\n",
      "Epoch [806/1200], Loss: 284.9947\n",
      "Epoch [807/1200], Loss: 284.2371\n",
      "Epoch [808/1200], Loss: 283.5927\n",
      "Epoch [809/1200], Loss: 281.6226\n",
      "Epoch [810/1200], Loss: 280.0829\n",
      "Epoch [811/1200], Loss: 278.8474\n",
      "Epoch [812/1200], Loss: 277.7972\n",
      "Epoch [813/1200], Loss: 276.9936\n",
      "Epoch [814/1200], Loss: 276.5077\n",
      "Epoch [815/1200], Loss: 276.1619\n",
      "Epoch [816/1200], Loss: 276.2799\n",
      "Epoch [817/1200], Loss: 277.0933\n",
      "Epoch [818/1200], Loss: 288.5079\n",
      "Epoch [819/1200], Loss: 341.3444\n",
      "Epoch [820/1200], Loss: 628.6016\n",
      "Epoch [821/1200], Loss: 617.3413\n",
      "Epoch [822/1200], Loss: 698.5135\n",
      "Epoch [823/1200], Loss: 1117.7883\n",
      "Epoch [824/1200], Loss: 627.7977\n",
      "Epoch [825/1200], Loss: 762.6803\n",
      "Epoch [826/1200], Loss: 779.5000\n",
      "Epoch [827/1200], Loss: 618.5868\n",
      "Epoch [828/1200], Loss: 665.2501\n",
      "Epoch [829/1200], Loss: 645.3451\n",
      "Epoch [830/1200], Loss: 554.5816\n",
      "Epoch [831/1200], Loss: 606.0042\n",
      "Epoch [832/1200], Loss: 540.9845\n",
      "Epoch [833/1200], Loss: 480.3660\n",
      "Epoch [834/1200], Loss: 499.0635\n",
      "Epoch [835/1200], Loss: 512.1700\n",
      "Epoch [836/1200], Loss: 439.4845\n",
      "Epoch [837/1200], Loss: 442.3760\n",
      "Epoch [838/1200], Loss: 460.5075\n",
      "Epoch [839/1200], Loss: 385.5022\n",
      "Epoch [840/1200], Loss: 384.7864\n",
      "Epoch [841/1200], Loss: 416.5515\n",
      "Epoch [842/1200], Loss: 411.8342\n",
      "Epoch [843/1200], Loss: 374.7194\n",
      "Epoch [844/1200], Loss: 363.7454\n",
      "Epoch [845/1200], Loss: 370.3689\n",
      "Epoch [846/1200], Loss: 354.3435\n",
      "Epoch [847/1200], Loss: 356.2591\n",
      "Epoch [848/1200], Loss: 343.7876\n",
      "Epoch [849/1200], Loss: 336.5191\n",
      "Epoch [850/1200], Loss: 334.6239\n",
      "Epoch [851/1200], Loss: 331.1649\n",
      "Epoch [852/1200], Loss: 320.6638\n",
      "Epoch [853/1200], Loss: 322.7058\n",
      "Epoch [854/1200], Loss: 318.2327\n",
      "Epoch [855/1200], Loss: 303.6828\n",
      "Epoch [856/1200], Loss: 307.7802\n",
      "Epoch [857/1200], Loss: 304.5262\n",
      "Epoch [858/1200], Loss: 295.4036\n",
      "Epoch [859/1200], Loss: 297.5858\n",
      "Epoch [860/1200], Loss: 293.7371\n",
      "Epoch [861/1200], Loss: 286.2197\n",
      "Epoch [862/1200], Loss: 288.9073\n",
      "Epoch [863/1200], Loss: 288.0946\n",
      "Epoch [864/1200], Loss: 280.7456\n",
      "Epoch [865/1200], Loss: 281.6429\n",
      "Epoch [866/1200], Loss: 280.5795\n",
      "Epoch [867/1200], Loss: 277.5592\n",
      "Epoch [868/1200], Loss: 277.5264\n",
      "Epoch [869/1200], Loss: 274.6642\n",
      "Epoch [870/1200], Loss: 272.3000\n",
      "Epoch [871/1200], Loss: 272.3203\n",
      "Epoch [872/1200], Loss: 271.4012\n",
      "Epoch [873/1200], Loss: 269.0031\n",
      "Epoch [874/1200], Loss: 268.5815\n",
      "Epoch [875/1200], Loss: 267.8952\n",
      "Epoch [876/1200], Loss: 266.2412\n",
      "Epoch [877/1200], Loss: 265.7804\n",
      "Epoch [878/1200], Loss: 263.8449\n",
      "Epoch [879/1200], Loss: 262.4310\n",
      "Epoch [880/1200], Loss: 262.2122\n",
      "Epoch [881/1200], Loss: 261.4146\n",
      "Epoch [882/1200], Loss: 260.0719\n",
      "Epoch [883/1200], Loss: 259.3678\n",
      "Epoch [884/1200], Loss: 258.5892\n",
      "Epoch [885/1200], Loss: 257.4337\n",
      "Epoch [886/1200], Loss: 256.7932\n",
      "Epoch [887/1200], Loss: 255.6121\n",
      "Epoch [888/1200], Loss: 254.8616\n",
      "Epoch [889/1200], Loss: 254.4705\n",
      "Epoch [890/1200], Loss: 253.4615\n",
      "Epoch [891/1200], Loss: 252.4058\n",
      "Epoch [892/1200], Loss: 251.8877\n",
      "Epoch [893/1200], Loss: 251.0683\n",
      "Epoch [894/1200], Loss: 250.4459\n",
      "Epoch [895/1200], Loss: 249.7644\n",
      "Epoch [896/1200], Loss: 248.9279\n",
      "Epoch [897/1200], Loss: 248.2757\n",
      "Epoch [898/1200], Loss: 247.4934\n",
      "Epoch [899/1200], Loss: 246.8429\n",
      "Epoch [900/1200], Loss: 246.2169\n",
      "Epoch [901/1200], Loss: 245.3501\n",
      "Epoch [902/1200], Loss: 244.8876\n",
      "Epoch [903/1200], Loss: 244.1420\n",
      "Epoch [904/1200], Loss: 243.5332\n",
      "Epoch [905/1200], Loss: 242.9332\n",
      "Epoch [906/1200], Loss: 242.2886\n",
      "Epoch [907/1200], Loss: 241.6979\n",
      "Epoch [908/1200], Loss: 241.0792\n",
      "Epoch [909/1200], Loss: 240.5520\n",
      "Epoch [910/1200], Loss: 239.8416\n",
      "Epoch [911/1200], Loss: 239.2849\n",
      "Epoch [912/1200], Loss: 238.7082\n",
      "Epoch [913/1200], Loss: 238.0131\n",
      "Epoch [914/1200], Loss: 237.5503\n",
      "Epoch [915/1200], Loss: 236.9007\n",
      "Epoch [916/1200], Loss: 236.3735\n",
      "Epoch [917/1200], Loss: 235.7657\n",
      "Epoch [918/1200], Loss: 235.2541\n",
      "Epoch [919/1200], Loss: 234.6754\n",
      "Epoch [920/1200], Loss: 234.1482\n",
      "Epoch [921/1200], Loss: 233.4121\n",
      "Epoch [922/1200], Loss: 232.9538\n",
      "Epoch [923/1200], Loss: 232.4693\n",
      "Epoch [924/1200], Loss: 231.7572\n",
      "Epoch [925/1200], Loss: 231.2804\n",
      "Epoch [926/1200], Loss: 230.6986\n",
      "Epoch [927/1200], Loss: 230.1224\n",
      "Epoch [928/1200], Loss: 229.5401\n",
      "Epoch [929/1200], Loss: 228.9234\n",
      "Epoch [930/1200], Loss: 228.4270\n",
      "Epoch [931/1200], Loss: 227.8595\n",
      "Epoch [932/1200], Loss: 227.2644\n",
      "Epoch [933/1200], Loss: 226.7004\n",
      "Epoch [934/1200], Loss: 226.1254\n",
      "Epoch [935/1200], Loss: 225.7157\n",
      "Epoch [936/1200], Loss: 225.0835\n",
      "Epoch [937/1200], Loss: 224.4623\n",
      "Epoch [938/1200], Loss: 224.1175\n",
      "Epoch [939/1200], Loss: 223.3974\n",
      "Epoch [940/1200], Loss: 222.9395\n",
      "Epoch [941/1200], Loss: 222.4540\n",
      "Epoch [942/1200], Loss: 221.9791\n",
      "Epoch [943/1200], Loss: 221.4616\n",
      "Epoch [944/1200], Loss: 220.9235\n",
      "Epoch [945/1200], Loss: 220.3896\n",
      "Epoch [946/1200], Loss: 219.9579\n",
      "Epoch [947/1200], Loss: 219.5157\n",
      "Epoch [948/1200], Loss: 219.0624\n",
      "Epoch [949/1200], Loss: 218.4468\n",
      "Epoch [950/1200], Loss: 217.9279\n",
      "Epoch [951/1200], Loss: 217.3479\n",
      "Epoch [952/1200], Loss: 216.8482\n",
      "Epoch [953/1200], Loss: 216.3439\n",
      "Epoch [954/1200], Loss: 215.8255\n",
      "Epoch [955/1200], Loss: 215.3018\n",
      "Epoch [956/1200], Loss: 214.8954\n",
      "Epoch [957/1200], Loss: 214.3932\n",
      "Epoch [958/1200], Loss: 213.8386\n",
      "Epoch [959/1200], Loss: 213.3679\n",
      "Epoch [960/1200], Loss: 212.7993\n",
      "Epoch [961/1200], Loss: 212.3572\n",
      "Epoch [962/1200], Loss: 211.8387\n",
      "Epoch [963/1200], Loss: 211.3376\n",
      "Epoch [964/1200], Loss: 210.9300\n",
      "Epoch [965/1200], Loss: 210.3209\n",
      "Epoch [966/1200], Loss: 209.8945\n",
      "Epoch [967/1200], Loss: 209.2822\n",
      "Epoch [968/1200], Loss: 208.8136\n",
      "Epoch [969/1200], Loss: 208.3137\n",
      "Epoch [970/1200], Loss: 207.8378\n",
      "Epoch [971/1200], Loss: 207.2390\n",
      "Epoch [972/1200], Loss: 206.6886\n",
      "Epoch [973/1200], Loss: 206.3094\n",
      "Epoch [974/1200], Loss: 205.7719\n",
      "Epoch [975/1200], Loss: 205.2386\n",
      "Epoch [976/1200], Loss: 204.8050\n",
      "Epoch [977/1200], Loss: 204.1439\n",
      "Epoch [978/1200], Loss: 203.7707\n",
      "Epoch [979/1200], Loss: 203.0812\n",
      "Epoch [980/1200], Loss: 202.6317\n",
      "Epoch [981/1200], Loss: 202.0582\n",
      "Epoch [982/1200], Loss: 201.5395\n",
      "Epoch [983/1200], Loss: 200.9367\n",
      "Epoch [984/1200], Loss: 200.4852\n",
      "Epoch [985/1200], Loss: 199.8940\n",
      "Epoch [986/1200], Loss: 199.4385\n",
      "Epoch [987/1200], Loss: 198.9038\n",
      "Epoch [988/1200], Loss: 198.3636\n",
      "Epoch [989/1200], Loss: 197.8776\n",
      "Epoch [990/1200], Loss: 197.3011\n",
      "Epoch [991/1200], Loss: 196.8642\n",
      "Epoch [992/1200], Loss: 196.2823\n",
      "Epoch [993/1200], Loss: 195.6137\n",
      "Epoch [994/1200], Loss: 195.2199\n",
      "Epoch [995/1200], Loss: 194.8994\n",
      "Epoch [996/1200], Loss: 193.9161\n",
      "Epoch [997/1200], Loss: 193.4947\n",
      "Epoch [998/1200], Loss: 193.1264\n",
      "Epoch [999/1200], Loss: 192.3531\n",
      "Epoch [1000/1200], Loss: 191.7018\n",
      "Epoch [1001/1200], Loss: 191.1887\n",
      "Epoch [1002/1200], Loss: 190.7256\n",
      "Epoch [1003/1200], Loss: 190.0847\n",
      "Epoch [1004/1200], Loss: 189.4568\n",
      "Epoch [1005/1200], Loss: 189.0839\n",
      "Epoch [1006/1200], Loss: 188.6510\n",
      "Epoch [1007/1200], Loss: 188.1846\n",
      "Epoch [1008/1200], Loss: 187.5035\n",
      "Epoch [1009/1200], Loss: 187.1001\n",
      "Epoch [1010/1200], Loss: 186.7000\n",
      "Epoch [1011/1200], Loss: 186.2136\n",
      "Epoch [1012/1200], Loss: 185.7297\n",
      "Epoch [1013/1200], Loss: 185.1846\n",
      "Epoch [1014/1200], Loss: 184.6936\n",
      "Epoch [1015/1200], Loss: 184.2246\n",
      "Epoch [1016/1200], Loss: 183.6959\n",
      "Epoch [1017/1200], Loss: 183.2944\n",
      "Epoch [1018/1200], Loss: 182.7750\n",
      "Epoch [1019/1200], Loss: 182.2989\n",
      "Epoch [1020/1200], Loss: 181.9743\n",
      "Epoch [1021/1200], Loss: 181.5891\n",
      "Epoch [1022/1200], Loss: 181.3074\n",
      "Epoch [1023/1200], Loss: 180.6162\n",
      "Epoch [1024/1200], Loss: 180.1894\n",
      "Epoch [1025/1200], Loss: 179.5731\n",
      "Epoch [1026/1200], Loss: 179.1397\n",
      "Epoch [1027/1200], Loss: 178.5288\n",
      "Epoch [1028/1200], Loss: 178.2928\n",
      "Epoch [1029/1200], Loss: 177.6992\n",
      "Epoch [1030/1200], Loss: 177.3126\n",
      "Epoch [1031/1200], Loss: 176.9375\n",
      "Epoch [1032/1200], Loss: 176.5219\n",
      "Epoch [1033/1200], Loss: 175.8922\n",
      "Epoch [1034/1200], Loss: 175.4086\n",
      "Epoch [1035/1200], Loss: 175.1670\n",
      "Epoch [1036/1200], Loss: 174.8176\n",
      "Epoch [1037/1200], Loss: 174.2223\n",
      "Epoch [1038/1200], Loss: 173.7272\n",
      "Epoch [1039/1200], Loss: 173.4677\n",
      "Epoch [1040/1200], Loss: 173.4232\n",
      "Epoch [1041/1200], Loss: 172.3865\n",
      "Epoch [1042/1200], Loss: 171.9393\n",
      "Epoch [1043/1200], Loss: 171.7477\n",
      "Epoch [1044/1200], Loss: 171.1775\n",
      "Epoch [1045/1200], Loss: 170.6183\n",
      "Epoch [1046/1200], Loss: 170.1739\n",
      "Epoch [1047/1200], Loss: 170.1809\n",
      "Epoch [1048/1200], Loss: 170.0884\n",
      "Epoch [1049/1200], Loss: 168.8941\n",
      "Epoch [1050/1200], Loss: 168.3322\n",
      "Epoch [1051/1200], Loss: 168.0862\n",
      "Epoch [1052/1200], Loss: 167.6515\n",
      "Epoch [1053/1200], Loss: 167.2953\n",
      "Epoch [1054/1200], Loss: 166.6810\n",
      "Epoch [1055/1200], Loss: 166.2509\n",
      "Epoch [1056/1200], Loss: 165.8705\n",
      "Epoch [1057/1200], Loss: 165.3549\n",
      "Epoch [1058/1200], Loss: 164.9286\n",
      "Epoch [1059/1200], Loss: 164.5910\n",
      "Epoch [1060/1200], Loss: 164.1060\n",
      "Epoch [1061/1200], Loss: 163.8000\n",
      "Epoch [1062/1200], Loss: 163.5207\n",
      "Epoch [1063/1200], Loss: 162.9163\n",
      "Epoch [1064/1200], Loss: 162.4668\n",
      "Epoch [1065/1200], Loss: 162.0838\n",
      "Epoch [1066/1200], Loss: 161.5560\n",
      "Epoch [1067/1200], Loss: 161.1410\n",
      "Epoch [1068/1200], Loss: 160.6540\n",
      "Epoch [1069/1200], Loss: 160.0756\n",
      "Epoch [1070/1200], Loss: 159.4570\n",
      "Epoch [1071/1200], Loss: 159.0185\n",
      "Epoch [1072/1200], Loss: 158.7614\n",
      "Epoch [1073/1200], Loss: 158.1268\n",
      "Epoch [1074/1200], Loss: 157.3804\n",
      "Epoch [1075/1200], Loss: 156.9035\n",
      "Epoch [1076/1200], Loss: 156.4844\n",
      "Epoch [1077/1200], Loss: 156.0486\n",
      "Epoch [1078/1200], Loss: 155.6097\n",
      "Epoch [1079/1200], Loss: 155.3226\n",
      "Epoch [1080/1200], Loss: 154.6184\n",
      "Epoch [1081/1200], Loss: 154.0128\n",
      "Epoch [1082/1200], Loss: 153.5446\n",
      "Epoch [1083/1200], Loss: 153.1153\n",
      "Epoch [1084/1200], Loss: 152.6813\n",
      "Epoch [1085/1200], Loss: 152.2822\n",
      "Epoch [1086/1200], Loss: 151.7568\n",
      "Epoch [1087/1200], Loss: 151.4024\n",
      "Epoch [1088/1200], Loss: 150.9992\n",
      "Epoch [1089/1200], Loss: 150.4189\n",
      "Epoch [1090/1200], Loss: 150.1022\n",
      "Epoch [1091/1200], Loss: 149.7933\n",
      "Epoch [1092/1200], Loss: 149.2309\n",
      "Epoch [1093/1200], Loss: 148.6577\n",
      "Epoch [1094/1200], Loss: 148.4944\n",
      "Epoch [1095/1200], Loss: 148.3451\n",
      "Epoch [1096/1200], Loss: 147.7343\n",
      "Epoch [1097/1200], Loss: 146.9936\n",
      "Epoch [1098/1200], Loss: 146.8546\n",
      "Epoch [1099/1200], Loss: 146.8528\n",
      "Epoch [1100/1200], Loss: 145.9504\n",
      "Epoch [1101/1200], Loss: 145.5138\n",
      "Epoch [1102/1200], Loss: 145.2444\n",
      "Epoch [1103/1200], Loss: 144.8617\n",
      "Epoch [1104/1200], Loss: 144.3507\n",
      "Epoch [1105/1200], Loss: 143.6961\n",
      "Epoch [1106/1200], Loss: 143.9190\n",
      "Epoch [1107/1200], Loss: 142.9623\n",
      "Epoch [1108/1200], Loss: 142.7274\n",
      "Epoch [1109/1200], Loss: 142.2259\n",
      "Epoch [1110/1200], Loss: 142.1431\n",
      "Epoch [1111/1200], Loss: 141.4752\n",
      "Epoch [1112/1200], Loss: 140.9751\n",
      "Epoch [1113/1200], Loss: 140.9120\n",
      "Epoch [1114/1200], Loss: 140.3360\n",
      "Epoch [1115/1200], Loss: 139.9893\n",
      "Epoch [1116/1200], Loss: 139.3474\n",
      "Epoch [1117/1200], Loss: 139.1472\n",
      "Epoch [1118/1200], Loss: 139.1046\n",
      "Epoch [1119/1200], Loss: 138.4352\n",
      "Epoch [1120/1200], Loss: 138.0383\n",
      "Epoch [1121/1200], Loss: 137.5306\n",
      "Epoch [1122/1200], Loss: 137.2997\n",
      "Epoch [1123/1200], Loss: 137.0134\n",
      "Epoch [1124/1200], Loss: 136.5355\n",
      "Epoch [1125/1200], Loss: 136.0272\n",
      "Epoch [1126/1200], Loss: 135.6955\n",
      "Epoch [1127/1200], Loss: 135.3130\n",
      "Epoch [1128/1200], Loss: 134.9386\n",
      "Epoch [1129/1200], Loss: 134.5845\n",
      "Epoch [1130/1200], Loss: 134.3622\n",
      "Epoch [1131/1200], Loss: 133.8778\n",
      "Epoch [1132/1200], Loss: 133.4141\n",
      "Epoch [1133/1200], Loss: 133.0558\n",
      "Epoch [1134/1200], Loss: 132.9715\n",
      "Epoch [1135/1200], Loss: 132.5023\n",
      "Epoch [1136/1200], Loss: 132.2201\n",
      "Epoch [1137/1200], Loss: 131.8391\n",
      "Epoch [1138/1200], Loss: 131.3284\n",
      "Epoch [1139/1200], Loss: 131.0450\n",
      "Epoch [1140/1200], Loss: 130.6818\n",
      "Epoch [1141/1200], Loss: 130.4073\n",
      "Epoch [1142/1200], Loss: 130.0782\n",
      "Epoch [1143/1200], Loss: 129.6840\n",
      "Epoch [1144/1200], Loss: 129.3176\n",
      "Epoch [1145/1200], Loss: 128.8922\n",
      "Epoch [1146/1200], Loss: 128.5953\n",
      "Epoch [1147/1200], Loss: 128.2982\n",
      "Epoch [1148/1200], Loss: 128.2633\n",
      "Epoch [1149/1200], Loss: 127.8280\n",
      "Epoch [1150/1200], Loss: 127.2731\n",
      "Epoch [1151/1200], Loss: 126.9847\n",
      "Epoch [1152/1200], Loss: 126.8496\n",
      "Epoch [1153/1200], Loss: 126.3856\n",
      "Epoch [1154/1200], Loss: 126.0482\n",
      "Epoch [1155/1200], Loss: 125.6081\n",
      "Epoch [1156/1200], Loss: 125.3394\n",
      "Epoch [1157/1200], Loss: 125.0992\n",
      "Epoch [1158/1200], Loss: 124.6918\n",
      "Epoch [1159/1200], Loss: 124.3316\n",
      "Epoch [1160/1200], Loss: 123.9944\n",
      "Epoch [1161/1200], Loss: 123.5676\n",
      "Epoch [1162/1200], Loss: 123.4613\n",
      "Epoch [1163/1200], Loss: 123.1218\n",
      "Epoch [1164/1200], Loss: 122.7056\n",
      "Epoch [1165/1200], Loss: 122.6143\n",
      "Epoch [1166/1200], Loss: 122.0608\n",
      "Epoch [1167/1200], Loss: 122.0678\n",
      "Epoch [1168/1200], Loss: 121.7699\n",
      "Epoch [1169/1200], Loss: 121.4979\n",
      "Epoch [1170/1200], Loss: 120.8933\n",
      "Epoch [1171/1200], Loss: 120.7438\n",
      "Epoch [1172/1200], Loss: 120.5986\n",
      "Epoch [1173/1200], Loss: 120.0416\n",
      "Epoch [1174/1200], Loss: 119.6891\n",
      "Epoch [1175/1200], Loss: 119.5335\n",
      "Epoch [1176/1200], Loss: 119.2761\n",
      "Epoch [1177/1200], Loss: 118.8866\n",
      "Epoch [1178/1200], Loss: 118.4746\n",
      "Epoch [1179/1200], Loss: 118.1603\n",
      "Epoch [1180/1200], Loss: 118.1349\n",
      "Epoch [1181/1200], Loss: 117.9821\n",
      "Epoch [1182/1200], Loss: 117.2324\n",
      "Epoch [1183/1200], Loss: 116.9979\n",
      "Epoch [1184/1200], Loss: 116.7465\n",
      "Epoch [1185/1200], Loss: 116.5802\n",
      "Epoch [1186/1200], Loss: 116.1329\n",
      "Epoch [1187/1200], Loss: 115.7151\n",
      "Epoch [1188/1200], Loss: 115.5705\n",
      "Epoch [1189/1200], Loss: 115.4299\n",
      "Epoch [1190/1200], Loss: 115.1909\n",
      "Epoch [1191/1200], Loss: 114.6335\n",
      "Epoch [1192/1200], Loss: 114.3596\n",
      "Epoch [1193/1200], Loss: 114.2480\n",
      "Epoch [1194/1200], Loss: 114.0408\n",
      "Epoch [1195/1200], Loss: 113.4378\n",
      "Epoch [1196/1200], Loss: 113.2502\n",
      "Epoch [1197/1200], Loss: 113.2280\n",
      "Epoch [1198/1200], Loss: 112.8510\n",
      "Epoch [1199/1200], Loss: 112.3467\n",
      "Epoch [1200/1200], Loss: 112.0697\n"
     ]
    }
   ],
   "source": [
    "loss, classifier = train(classifier=classifier, dataloader=dataloader, criterion=nn.CrossEntropyLoss(reduction=\"sum\"), optimizer=torch.optim.Adam(classifier.parameters(), lr=LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss)           \n",
    "plt.title(\"Line Chart\")  \n",
    "plt.xlabel(\"epoch\")     \n",
    "plt.ylabel(\"loss\")      \n",
    "plt.show()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicción: 1\n",
      "Etiqueta real: 5\n"
     ]
    }
   ],
   "source": [
    "def prediction(image):\n",
    "    # Envía la imagen a GPU, obtiene logits y selecciona la clase predicha.\n",
    "    output = classifier(image.to(\"cuda\"))\n",
    "    pred_class = torch.argmax(output, dim=1)\n",
    "    return pred_class\n",
    "\n",
    "# Seleccionar una imagen aleatoria del conjunto de prueba\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "image, label = test_set[idx]\n",
    "\n",
    "# Evaluar sin calcular gradientes\n",
    "with torch.no_grad():\n",
    "    pred = prediction(image)\n",
    "\n",
    "# Convertir etiqueta one-hot a índice si es necesario\n",
    "if label.numel() > 1:\n",
    "    true_class = torch.argmax(label)\n",
    "else:\n",
    "    true_class = label\n",
    "\n",
    "print(\"Predicción:\", pred.item())\n",
    "print(\"Etiqueta real:\", true_class.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 0.88003663003663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Listas para almacenar etiquetas verdaderas y predichas\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "# Colocar el modelo en modo evaluación\n",
    "classifier.eval()\n",
    "\n",
    "# Desactivar cálculo de gradientes durante la evaluación\n",
    "with torch.no_grad():\n",
    "    for image, label in test_set:\n",
    "        # Obtener la predicción (se asume que 'prediction' ya mueve la imagen a GPU)\n",
    "        pred = prediction(image)\n",
    "        \n",
    "        # Convertir la etiqueta one-hot a índice, si es necesario\n",
    "        if isinstance(label, torch.Tensor):\n",
    "            label_idx = torch.argmax(label).item() if label.numel() > 1 else label.item()\n",
    "        else:\n",
    "            label_idx = label\n",
    "        \n",
    "        true_labels.append(label_idx)\n",
    "        pred_labels.append(pred.item())\n",
    "\n",
    "# Calcular la precisión (macro average)\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')\n",
    "print(\"Precisión del modelo:\", precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  4,\n",
       "  3,\n",
       "  0,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  0,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  0,\n",
       "  0,\n",
       "  5,\n",
       "  2,\n",
       "  0,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  2],\n",
       " [4,\n",
       "  4,\n",
       "  1,\n",
       "  2,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  1,\n",
       "  4,\n",
       "  4,\n",
       "  3,\n",
       "  3,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  2,\n",
       "  5,\n",
       "  3,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  0,\n",
       "  2,\n",
       "  1,\n",
       "  3,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  2,\n",
       "  3,\n",
       "  2,\n",
       "  4,\n",
       "  1,\n",
       "  5,\n",
       "  3,\n",
       "  4,\n",
       "  2,\n",
       "  4,\n",
       "  5,\n",
       "  5,\n",
       "  4,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  4,\n",
       "  3,\n",
       "  2,\n",
       "  1,\n",
       "  5,\n",
       "  1,\n",
       "  2,\n",
       "  3,\n",
       "  3,\n",
       "  4,\n",
       "  4,\n",
       "  2,\n",
       "  3,\n",
       "  4,\n",
       "  5,\n",
       "  2,\n",
       "  3,\n",
       "  2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels, pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification_metrics(df):\n",
    "    \"\"\"\n",
    "    Calcula por clase (one-vs-rest):\n",
    "      - Precision, recall, f1_score, accuracy y AUC ROC.\n",
    "    Retorna un dataframe donde cada fila es una clase y la última fila es el promedio.\n",
    "    \"\"\"\n",
    "    # Extraer etiquetas y clases\n",
    "    y_true = df['GroundTruth'].values\n",
    "    y_pred = df['Prediction'].values\n",
    "    classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    \n",
    "    # Binarizar etiquetas para one-vs-rest\n",
    "    y_true_bin = label_binarize(y_true, classes=classes)\n",
    "    y_pred_bin = label_binarize(y_pred, classes=classes)\n",
    "    \n",
    "    records = []\n",
    "    for i, cls in enumerate(classes):\n",
    "        # Calcular métricas\n",
    "        prec = precision_score(y_true_bin[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "        rec = recall_score(y_true_bin[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true_bin[:, i], y_pred_bin[:, i], zero_division=0)\n",
    "        acc = accuracy_score(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "        # AUC ROC requiere ambas clases en y_true\n",
    "        auc = (roc_auc_score(y_true_bin[:, i], y_pred_bin[:, i])\n",
    "               if len(np.unique(y_true_bin[:, i])) > 1 else np.nan)\n",
    "        \n",
    "        records.append({\n",
    "            'Clase': cls,\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1_score': f1,\n",
    "            'Accuracy': acc,\n",
    "            'AUC_ROC': auc\n",
    "        })\n",
    "    \n",
    "    # Crear dataframe y agregar fila de promedio (macro)\n",
    "    df_metrics = pd.DataFrame(records).set_index('Clase')\n",
    "    df_metrics.loc['Promedio'] = df_metrics.mean(skipna=True)\n",
    "    return df_metrics\n",
    "\n",
    "def evaluate_model(y_pred: np.array, y_test: np.array, class_names_str: list):\n",
    "    df_result = pd.DataFrame({\"Prediction\": y_pred, \"GroundTruth\": y_test})\n",
    "    df_result[\"Prediction\"] = df_result[\"Prediction\"].apply(lambda x: class_names_str[x])\n",
    "    df_result[\"GroundTruth\"] = df_result[\"GroundTruth\"].apply(lambda x: class_names_str[x])\n",
    "    return evaluate_classification_metrics(df_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>Rolled</th>\n",
       "      <th>Patches</th>\n",
       "      <th>Scratches</th>\n",
       "      <th>Inclusion</th>\n",
       "      <th>Pitted</th>\n",
       "      <th>Crazing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>metal_dataset/train/Pitted/PS_262.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>metal_dataset/train/Inclusion/In_25.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>metal_dataset/train/Crazing/Cr_190.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metal_dataset/train/Inclusion/In_182.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>metal_dataset/train/Crazing/Cr_227.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>metal_dataset/train/Scratches/Sc_35.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>metal_dataset/train/Inclusion/In_6.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>metal_dataset/train/Inclusion/In_291.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>metal_dataset/train/Scratches/Sc_154.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>metal_dataset/train/Patches/Pa_203.bmp</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1656 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Path  Rolled  Patches  Scratches  \\\n",
       "0        metal_dataset/train/Pitted/PS_262.bmp       0        0          0   \n",
       "1      metal_dataset/train/Inclusion/In_25.bmp       0        0          0   \n",
       "2       metal_dataset/train/Crazing/Cr_190.bmp       0        0          0   \n",
       "3     metal_dataset/train/Inclusion/In_182.bmp       0        0          0   \n",
       "4       metal_dataset/train/Crazing/Cr_227.bmp       0        0          0   \n",
       "...                                        ...     ...      ...        ...   \n",
       "1651   metal_dataset/train/Scratches/Sc_35.bmp       0        0          1   \n",
       "1652    metal_dataset/train/Inclusion/In_6.bmp       0        0          0   \n",
       "1653  metal_dataset/train/Inclusion/In_291.bmp       0        0          0   \n",
       "1654  metal_dataset/train/Scratches/Sc_154.bmp       0        0          1   \n",
       "1655    metal_dataset/train/Patches/Pa_203.bmp       0        1          0   \n",
       "\n",
       "      Inclusion  Pitted  Crazing  \n",
       "0             0       1        0  \n",
       "1             1       0        0  \n",
       "2             0       0        1  \n",
       "3             1       0        0  \n",
       "4             0       0        1  \n",
       "...         ...     ...      ...  \n",
       "1651          0       0        0  \n",
       "1652          1       0        0  \n",
       "1653          1       0        0  \n",
       "1654          0       0        0  \n",
       "1655          0       0        0  \n",
       "\n",
       "[1656 rows x 7 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC_ROC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clase</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Crazing</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inclusion</th>\n",
       "      <td>0.923077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Patches</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pitted</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.956522</td>\n",
       "      <td>0.986111</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rolled</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scratches</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Promedio</th>\n",
       "      <td>0.880037</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.874782</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.925000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Precision    Recall  F1_score  Accuracy   AUC_ROC\n",
       "Clase                                                       \n",
       "Crazing     0.857143  1.000000  0.923077  0.972222  0.983333\n",
       "Inclusion   0.923077  1.000000  0.960000  0.986111  0.991667\n",
       "Patches     1.000000  0.833333  0.909091  0.972222  0.916667\n",
       "Pitted      1.000000  0.916667  0.956522  0.986111  0.958333\n",
       "Rolled      0.750000  0.750000  0.750000  0.916667  0.850000\n",
       "Scratches   0.750000  0.750000  0.750000  0.916667  0.850000\n",
       "Promedio    0.880037  0.875000  0.874782  0.958333  0.925000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(y_pred=np.array(pred_labels), y_test=np.array(true_labels), class_names_str=[\"Rolled\", \"Patches\", \"Scratches\", \"Inclusion\", \"Pitted\", \"Crazing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardar solo la red neuronal en un archivo .pth\n",
    "torch.save(classifier.state_dict(), \"classifier.pth\")\n",
    "print(\"Modelo guardado exitosamente en classifier.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
